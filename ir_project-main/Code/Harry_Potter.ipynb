{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "969uozarNohq"
      },
      "source": [
        "# Harry Potter Data Analysis\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heFPPLYE1FOD",
        "outputId": "53d3f337-863e-40ed-fc63-d3531e6d3c6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.12.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.69.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "#!pip install numpy pandas matplotlib seaborn\n",
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zYgBapP1s8X"
      },
      "source": [
        "# Step 1: Data Loading and Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwuheZeT1yIp"
      },
      "source": [
        "The following code loads various Harry Potter-related datasets from CSV files. These datasets contain information about characters, dialogues, potions, and spells. After loading, the code standardizes column names, handles missing values, and cleans text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2L1fNMZ2m_-"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "03zkzgpt1h0h"
      },
      "outputs": [],
      "source": [
        "# Core Libraries\n",
        "import numpy as np  # For numerical operations and array handling\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import json  # For handling JSON data\n",
        "from collections import defaultdict, Counter  # For handling grouped data and counting elements\n",
        "\n",
        "# Visualization Libraries\n",
        "import matplotlib.pyplot as plt  # For creating static plots\n",
        "import seaborn as sns  # For high-level statistical graphics\n",
        "\n",
        "# NLP Libraries (Natural Language Processing)\n",
        "import re  # For regular expression operations (text cleaning)\n",
        "import nltk  # For working with human language data\n",
        "from nltk.corpus import stopwords, wordnet  # Stopwords for filtering, wordnet for lemmatization\n",
        "from nltk.tokenize import word_tokenize  # For splitting text into tokens (words)\n",
        "from nltk import pos_tag  # For Part-of-Speech tagging\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer  # For stemming and lemmatization\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV  # For splitting data and hyperparameter tuning\n",
        "from sklearn.metrics.pairwise import cosine_similarity  # For calculating pairwise cosine similarity between vectors\n",
        "import math  # For mathematical functions like log\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # For transforming text data into TF-IDF features\n",
        "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes classifier for text classification\n",
        "from sklearn.linear_model import LogisticRegression  # Logistic Regression classifier\n",
        "from sklearn.svm import SVC  # Support Vector Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier  # Random Forest classifier\n",
        "from sklearn.pipeline import Pipeline  # For creating ML pipelines\n",
        "from sklearn.metrics import classification_report, accuracy_score  # For model evaluation metrics\n",
        "from sklearn.preprocessing import LabelEncoder  # For encoding categorical labels\n",
        "\n",
        "# Imbalanced Data Handling Libraries\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline  # Pipeline supporting imbalanced data\n",
        "from imblearn.over_sampling import SMOTE  # For over-sampling minority class\n",
        "from imblearn.under_sampling import RandomUnderSampler  # For under-sampling majority class\n",
        "\n",
        "# Deep Learning Libraries\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # For tokenizing text data\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # For padding sequences to equal length\n",
        "from tensorflow.keras.models import Sequential  # For building sequential models\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense  # For creating deep learning layers\n",
        "from tensorflow.keras.utils import to_categorical  # For converting labels to categorical format\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngkbTZrj24dN"
      },
      "source": [
        "#  Loading datasets with specific delimiters and encodings\n",
        "\n",
        "If you are working in Colab, as we did, it is necessary to upload the data to it. So in the end, we upload corpuses for all three movies, potions and spells.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "okL1ni8O2vGq"
      },
      "outputs": [],
      "source": [
        "# Loading datasets with specific delimiters and encodings\n",
        "# Each file contains Harry Potter-related data (dialogues, potions, spells, etc.)\n",
        "# 'latin1' for correctly loading text\n",
        "df1 = pd.read_csv('/content/Harry_Potter/Harry Potter 1.csv', delimiter=';', encoding='latin1')\n",
        "df2 = pd.read_csv('/content/Harry_Potter/Harry Potter 2.csv', delimiter=';', encoding='latin1')\n",
        "df3 = pd.read_csv('/content/Harry_Potter/Harry Potter 3.csv', delimiter=';', encoding='latin1')\n",
        "df4 = pd.read_csv('/content/Harry_Potter/Potions.csv', delimiter=';', encoding='latin1')\n",
        "df5 = pd.read_csv('/content/Harry_Potter/Spells.csv', delimiter=';', encoding='latin1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BJIPQ5hO8hA",
        "outputId": "d283cc86-0c96-4f5d-9bbb-4ad57606a9af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['ï»¿Character', 'Sentence'], dtype='object')\n",
            "Index(['ï»¿Character', 'Sentence'], dtype='object')\n",
            "Index(['ï»¿CHARACTER', 'SENTENCE'], dtype='object')\n",
            "Index(['ï»¿Name', 'Known ingredients', 'Effect', 'Characteristics',\n",
            "       'Difficulty level,,,,,,,,'],\n",
            "      dtype='object')\n",
            "Index(['ï»¿Name', 'Incantation', 'Type', 'Effect', 'Light'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# See the colnames\n",
        "print(df1.columns)\n",
        "print(df2.columns)\n",
        "print(df3.columns)\n",
        "print(df4.columns)\n",
        "print(df5.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9Ve9lZR3JzL"
      },
      "source": [
        "# Renaming specific columns for consistent naming across datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v2aCM8RA2--o"
      },
      "outputs": [],
      "source": [
        "# Removing encoding issues (e.g., ï»¿Character)\n",
        "df1.rename(columns={'ï»¿Character': 'CHARACTER'}, inplace=True)\n",
        "df2.rename(columns={'ï»¿Character': 'CHARACTER'}, inplace=True)\n",
        "df3.rename(columns={'ï»¿CHARACTER': 'CHARACTER'}, inplace=True)\n",
        "df4.rename(columns={'ï»¿Name': 'name', 'Difficulty level,,,,,,,,': 'difficulty_level'}, inplace=True)\n",
        "df5.rename(columns={'ï»¿Name': 'Name'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YszaYwb93cBb"
      },
      "source": [
        "# Displaying Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvwQ8CBA3OFB",
        "outputId": "59441ff8-9f00-4223-9293-c57db06f3d34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preview of Harry Potter 1 Dataset\n",
            "    CHARACTER                                           Sentence\n",
            "0  Dumbledore  I should've known that you would be here, Prof...\n",
            "1  McGonagall                Good evening, Professor Dumbledore.\n",
            "2  McGonagall                        Are the rumors true, Albus?\n",
            "3  Dumbledore                          I'm afraid so, professor.\n",
            "4  Dumbledore                              The good and the bad.\n",
            "Preview of Harry Potter 2 Dataset\n",
            "  CHARACTER                                           Sentence\n",
            "0    HARRY                     I canât let you out, Hedwig. \n",
            "1    HARRY   Iâm not allowed to use magic outside of scho...\n",
            "2    HARRY                         Besides, if Uncle Vernonâ¦\n",
            "3    VERNON                                      Harry Potter!\n",
            "4     HARRY                              Now youâve done it.\n",
            "Preview of Harry Potter 3 Dataset\n",
            "      CHARACTER          SENTENCE\n",
            "0         HARRY   Lumos Maxima...\n",
            "1         HARRY   Lumos Maxima...\n",
            "2         HARRY   Lumos Maxima...\n",
            "3         HARRY  Lumos... MAXIMA!\n",
            "4  AUNT PETUNIA     Harry! Harry!\n",
            "\n",
            "Preview of Potions Dataset\n",
            "                      name                                  Known ingredients  \\\n",
            "0            Ageing Potion                             Newt spleens , Bananas   \n",
            "1               Amortentia                                                NaN   \n",
            "2  Antidote to Veritaserum                                                NaN   \n",
            "3        Babbling Beverage                  Valerian sprigs, Aconite, Dittany   \n",
            "4  Baruffio's Brain Elixir  Leaping Toadstools, Frog Brains, Runespoor egg...   \n",
            "\n",
            "                                              Effect  \\\n",
            "0                           Ages drinker temporarily   \n",
            "1  Love PotionÂ that causes a powerful infatuatio...   \n",
            "2                Counters the effect ofÂ Veritaserum   \n",
            "3               Causes the drinker to speak nonsense   \n",
            "4              Allegedly increases one's brain power   \n",
            "\n",
            "                                     Characteristics difficulty_level  \n",
            "0                                              Green  Advanced,,,,,,,  \n",
            "1  Mother-of-pearl sheen, Spiralling steam, Scent...   Advanced,,,,,,  \n",
            "2                                                NaN         ,,,,,,,,  \n",
            "3                                                NaN           ,,,,,,  \n",
            "4                                    Green in colour            ,,,,,  \n",
            "\n",
            "Preview of Spells Dataset\n",
            "                                Name       Incantation                Type  \\\n",
            "0                    Summoning Charm             Accio               Charm   \n",
            "1                           Age Line           Unknown               Charm   \n",
            "2                 Water-Making Spell         Aguamenti  Charm, Conjuration   \n",
            "3   Launch an object up into the air  Alarte Ascendare               Charm   \n",
            "4  Albus Dumbledore's Forceful Spell           Unknown               Spell   \n",
            "\n",
            "                                              Effect     Light  \n",
            "0                                  Summons an object       NaN  \n",
            "1  Prevents people above or below a certain age f...      Blue  \n",
            "2                                    ConjuresÂ water  Icy blue  \n",
            "3                              Rockets target upward       Red  \n",
            "4                                        Great Force       NaN  \n"
          ]
        }
      ],
      "source": [
        "# Displaying the first few rows of each dataset to verify successful loading\n",
        "print(\"Preview of Harry Potter 1 Dataset\")\n",
        "print(df1.head())\n",
        "\n",
        "print(\"Preview of Harry Potter 2 Dataset\")\n",
        "print(df2.head())\n",
        "\n",
        "print(\"Preview of Harry Potter 3 Dataset\")\n",
        "print(df3.head())\n",
        "\n",
        "print(\"\\nPreview of Potions Dataset\")\n",
        "print(df4.head())\n",
        "\n",
        "print(\"\\nPreview of Spells Dataset\")\n",
        "print(df5.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tI3InlU4Frt",
        "outputId": "cd1202be-5f41-4c75-bade-0648e17eecd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1587 entries, 0 to 1586\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   CHARACTER  1587 non-null   object\n",
            " 1   Sentence   1587 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 24.9+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1700 entries, 0 to 1699\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   CHARACTER  1700 non-null   object\n",
            " 1   Sentence   1700 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 26.7+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1638 entries, 0 to 1637\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   CHARACTER  1638 non-null   object\n",
            " 1   SENTENCE   1638 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 25.7+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1700 entries, 0 to 1699\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   CHARACTER  1700 non-null   object\n",
            " 1   Sentence   1700 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 26.7+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 74 entries, 0 to 73\n",
            "Data columns (total 5 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   name               74 non-null     object\n",
            " 1   Known ingredients  42 non-null     object\n",
            " 2   Effect             67 non-null     object\n",
            " 3   Characteristics    40 non-null     object\n",
            " 4   difficulty_level   69 non-null     object\n",
            "dtypes: object(5)\n",
            "memory usage: 3.0+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Print the information summary of DataFrames including the index dtype and columns, non-null values, and memory usage\n",
        "\n",
        "print(df1.info())\n",
        "print(df2.info())\n",
        "print(df3.info())\n",
        "print(df2.info())\n",
        "print(df4.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5c-mfFp5tOK",
        "outputId": "1be6c352-e879-4060-c5b5-0a69070ca6ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       CHARACTER Sentence\n",
            "count       1587     1587\n",
            "unique        91     1551\n",
            "top        Harry    What?\n",
            "freq         155        4\n",
            "       CHARACTER  Sentence\n",
            "count       1700      1700\n",
            "unique        72      1627\n",
            "top        HARRY  Come on.\n",
            "freq         368         8\n",
            "       CHARACTER SENTENCE\n",
            "count       1638     1638\n",
            "unique        51     1531\n",
            "top        HARRY   Harry!\n",
            "freq         308        7\n",
            "       CHARACTER  Sentence\n",
            "count       1700      1700\n",
            "unique        72      1627\n",
            "top        HARRY  Come on.\n",
            "freq         368         8\n",
            "                 name                 Known ingredients  \\\n",
            "count              74                                42   \n",
            "unique             74                                41   \n",
            "top     Ageing Potion  Scurvy grass, Lovage, Sneezewort   \n",
            "freq                1                                 2   \n",
            "\n",
            "                                                   Effect Characteristics  \\\n",
            "count                                                  67              40   \n",
            "unique                                                 64              37   \n",
            "top     Causes the drinker to become infatuated with t...           Green   \n",
            "freq                                                    4               2   \n",
            "\n",
            "       difficulty_level  \n",
            "count                69  \n",
            "unique               22  \n",
            "top            ,,,,,,,,  \n",
            "freq                 26  \n"
          ]
        }
      ],
      "source": [
        "# Print statistical summaries of DataFrames\n",
        "\n",
        "print(df1.describe())\n",
        "print(df2.describe())\n",
        "print(df3.describe())\n",
        "print(df2.describe())\n",
        "print(df4.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENhI-jSkBVDl"
      },
      "source": [
        "## Column Standardization and Text Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0ndr9dzBYkZ"
      },
      "source": [
        "This section standardizes column names across all datasets for uniformity and prepares the data by handling missing values and cleaning the text. This ensures consistency in data processing for downstream tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2JGD0ea5wvs",
        "outputId": "ba2e2f6b-64bf-44da-8688-e37408282728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated column names for df1:\n",
            "Index(['character', 'sentence'], dtype='object') \n",
            "\n",
            "Updated column names for df2:\n",
            "Index(['character', 'sentence'], dtype='object') \n",
            "\n",
            "Updated column names for df3:\n",
            "Index(['character', 'sentence'], dtype='object') \n",
            "\n",
            "Updated column names for df4:\n",
            "Index(['name', 'known_ingredients', 'effect', 'characteristics',\n",
            "       'difficulty_level'],\n",
            "      dtype='object') \n",
            "\n",
            "Updated column names for df5:\n",
            "Index(['name', 'incantation', 'type', 'effect', 'light'], dtype='object') \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Standardizing column names across all datasets\n",
        "# Converts column names to lowercase, removes extra spaces, and replaces spaces with underscores\n",
        "for df, name in zip([df1, df2, df3, df4, df5], ['df1', 'df2', 'df3', 'df4', 'df5']):\n",
        "    df.columns = df.columns.str.lower().str.replace(\" \", \"_\").str.strip()\n",
        "    print(f\"Updated column names for {name}:\")\n",
        "    print(df.columns, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2NWDfOuCCgN"
      },
      "source": [
        "# Handling missing values in datasets\n",
        "- Categorical columns: Replace missing values with \"Unknown\"\n",
        "- Numerical columns: Replace missing values with 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW-Ow7NgBd5e",
        "outputId": "ed8c7a20-c180-4b52-e3bf-1b3cad80d28b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-43c26efe51bc>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[column].fillna(\"Unknown\", inplace=True)\n"
          ]
        }
      ],
      "source": [
        "for df in [df1, df2, df3, df4, df5]:\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype in ['object']:  # If column type is object (string)\n",
        "            df[column].fillna(\"Unknown\", inplace=True)\n",
        "        elif df[column].dtype in ['int64', 'float64']:  # If column type is numerical\n",
        "            df[column].fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzoLi82cCsSm"
      },
      "source": [
        "# Cleaning text in all object (string) columns\n",
        "Here we removes extra spaces, tabs, convert everything to lowercase, and strip whitespaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHFrl3sXCHU_",
        "outputId": "bb8a5553-46a7-45b5-82e1-607c5abb85de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preview of cleaned text for df1:\n",
            "    character                                           sentence\n",
            "0  dumbledore  i should've known that you would be here, prof...\n",
            "1  mcgonagall                good evening, professor dumbledore.\n",
            "2  mcgonagall                        are the rumors true, albus?\n",
            "3  dumbledore                          i'm afraid so, professor.\n",
            "4  dumbledore                              the good and the bad. \n",
            "\n",
            "Preview of cleaned text for df2:\n",
            "  character                                           sentence\n",
            "0     harry                     i canât let you out, hedwig.\n",
            "1     harry  iâm not allowed to use magic outside of school.\n",
            "2     harry                        besides, if uncle vernonâ¦\n",
            "3    vernon                                      harry potter!\n",
            "4     harry                              now youâve done it. \n",
            "\n",
            "Preview of cleaned text for df3:\n",
            "      character          sentence\n",
            "0         harry   lumos maxima...\n",
            "1         harry   lumos maxima...\n",
            "2         harry   lumos maxima...\n",
            "3         harry  lumos... maxima!\n",
            "4  aunt petunia     harry! harry! \n",
            "\n",
            "Preview of cleaned text for df4:\n",
            "                      name                                  known_ingredients  \\\n",
            "0            ageing potion                             newt spleens , bananas   \n",
            "1               amortentia                                            unknown   \n",
            "2  antidote to veritaserum                                            unknown   \n",
            "3        babbling beverage                  valerian sprigs, aconite, dittany   \n",
            "4  baruffio's brain elixir  leaping toadstools, frog brains, runespoor egg...   \n",
            "\n",
            "                                              effect  \\\n",
            "0                           ages drinker temporarily   \n",
            "1  love potionâ that causes a powerful infatuatio...   \n",
            "2                counters the effect ofâ veritaserum   \n",
            "3               causes the drinker to speak nonsense   \n",
            "4              allegedly increases one's brain power   \n",
            "\n",
            "                                     characteristics difficulty_level  \n",
            "0                                              green  advanced,,,,,,,  \n",
            "1  mother-of-pearl sheen, spiralling steam, scent...   advanced,,,,,,  \n",
            "2                                            unknown         ,,,,,,,,  \n",
            "3                                            unknown           ,,,,,,  \n",
            "4                                    green in colour            ,,,,,   \n",
            "\n",
            "Preview of cleaned text for df5:\n",
            "                                name       incantation                type  \\\n",
            "0                    summoning charm             accio               charm   \n",
            "1                           age line           unknown               charm   \n",
            "2                 water-making spell         aguamenti  charm, conjuration   \n",
            "3   launch an object up into the air  alarte ascendare               charm   \n",
            "4  albus dumbledore's forceful spell           unknown               spell   \n",
            "\n",
            "                                              effect     light  \n",
            "0                                  summons an object   unknown  \n",
            "1  prevents people above or below a certain age f...      blue  \n",
            "2                                    conjuresâ water  icy blue  \n",
            "3                              rockets target upward       red  \n",
            "4                                        great force   unknown   \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for df, name in zip([df1, df2, df3, df4, df5], ['df1', 'df2', 'df3', 'df4', 'df5']):\n",
        "    for column in df.select_dtypes(include=['object']).columns:\n",
        "        df[column] = df[column].apply(lambda x: re.sub(r'\\s+', ' ', str(x).lower().strip()))\n",
        "    print(f\"Preview of cleaned text for {name}:\")\n",
        "    print(df.head(), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rofQr7H0CwDk",
        "outputId": "697908aa-e19e-4acd-b5c4-367dd358d5cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preview of cleaned and filled text for df1:\n",
            "    character                                           sentence\n",
            "0  dumbledore  i should've known that you would be here, prof...\n",
            "1  mcgonagall                good evening, professor dumbledore.\n",
            "2  mcgonagall                        are the rumors true, albus?\n",
            "3  dumbledore                          i'm afraid so, professor.\n",
            "4  dumbledore                              the good and the bad. \n",
            "\n",
            "Preview of cleaned and filled text for df2:\n",
            "  character                                           sentence\n",
            "0     harry                     i canât let you out, hedwig.\n",
            "1     harry  iâm not allowed to use magic outside of school.\n",
            "2     harry                        besides, if uncle vernonâ¦\n",
            "3    vernon                                      harry potter!\n",
            "4     harry                              now youâve done it. \n",
            "\n",
            "Preview of cleaned and filled text for df3:\n",
            "      character          sentence\n",
            "0         harry   lumos maxima...\n",
            "1         harry   lumos maxima...\n",
            "2         harry   lumos maxima...\n",
            "3         harry  lumos... maxima!\n",
            "4  aunt petunia     harry! harry! \n",
            "\n",
            "Preview of cleaned and filled text for df4:\n",
            "                      name                                  known_ingredients  \\\n",
            "0            ageing potion                             newt spleens , bananas   \n",
            "1               amortentia                                            unknown   \n",
            "2  antidote to veritaserum                                            unknown   \n",
            "3        babbling beverage                  valerian sprigs, aconite, dittany   \n",
            "4  baruffio's brain elixir  leaping toadstools, frog brains, runespoor egg...   \n",
            "\n",
            "                                              effect  \\\n",
            "0                           ages drinker temporarily   \n",
            "1  love potionâ that causes a powerful infatuatio...   \n",
            "2                counters the effect ofâ veritaserum   \n",
            "3               causes the drinker to speak nonsense   \n",
            "4              allegedly increases one's brain power   \n",
            "\n",
            "                                     characteristics difficulty_level  \n",
            "0                                              green  advanced,,,,,,,  \n",
            "1  mother-of-pearl sheen, spiralling steam, scent...   advanced,,,,,,  \n",
            "2                                            unknown         ,,,,,,,,  \n",
            "3                                            unknown           ,,,,,,  \n",
            "4                                    green in colour            ,,,,,   \n",
            "\n",
            "Preview of cleaned and filled text for df5:\n",
            "                                name       incantation                type  \\\n",
            "0                    summoning charm             accio               charm   \n",
            "1                           age line           unknown               charm   \n",
            "2                 water-making spell         aguamenti  charm, conjuration   \n",
            "3   launch an object up into the air  alarte ascendare               charm   \n",
            "4  albus dumbledore's forceful spell           unknown               spell   \n",
            "\n",
            "                                              effect     light  \n",
            "0                                  summons an object   unknown  \n",
            "1  prevents people above or below a certain age f...      blue  \n",
            "2                                    conjuresâ water  icy blue  \n",
            "3                              rockets target upward       red  \n",
            "4                                        great force   unknown   \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define a function to handle missing values\n",
        "def handle_missing_values(df, replacements):\n",
        "    for column, replacement in replacements.items():\n",
        "        # Ensure column exists in the DataFrame before attempting to fill\n",
        "        if column in df.columns:\n",
        "            df[column] = df[column].fillna(replacement)\n",
        "    return df\n",
        "\n",
        "# Handling missing values for each dataset\n",
        "# Dataset 1, 2, 3: Character and Dialogue Columns\n",
        "char_replacements = {\n",
        "    'character': 'Unknown',  # Use lowercase to ensure compatibility\n",
        "    'sentence': 'No dialogue provided'\n",
        "}\n",
        "\n",
        "# Dataset 4: Potions\n",
        "potions_replacements = {\n",
        "    'known_ingredients': 'Unknown ingredients',\n",
        "    'effect': 'Effect not specified',\n",
        "    'characteristics': 'Characteristics not specified',\n",
        "    'difficulty_level': 'Unknown'\n",
        "}\n",
        "\n",
        "# Dataset 5: Spells\n",
        "spells_replacements = {\n",
        "    'incantation': 'Unknown incantation',\n",
        "    'type': 'Not specified',\n",
        "    'effect': 'Not specified',\n",
        "    'light': 'Not specified'\n",
        "}\n",
        "\n",
        "# Defining the datasets dictionary with your DataFrames\n",
        "datasets = {\n",
        "    'df1': df1,\n",
        "    'df2': df2,\n",
        "    'df3': df3,\n",
        "    'df4': df4,\n",
        "    'df5': df5\n",
        "}\n",
        "\n",
        "# Standardize column names to lowercase for uniformity\n",
        "for name, df in datasets.items():\n",
        "    df.columns = df.columns.str.lower()\n",
        "\n",
        "# Apply cleaning and missing value handling\n",
        "datasets['df1'] = handle_missing_values(datasets['df1'], char_replacements)\n",
        "datasets['df2'] = handle_missing_values(datasets['df2'], char_replacements)\n",
        "datasets['df3'] = handle_missing_values(datasets['df3'], char_replacements)\n",
        "datasets['df4'] = handle_missing_values(datasets['df4'], potions_replacements)\n",
        "datasets['df5'] = handle_missing_values(datasets['df5'], spells_replacements)\n",
        "\n",
        "# Display preview of cleaned data with missing values handled\n",
        "for name, df in datasets.items():\n",
        "    print(f\"Preview of cleaned and filled text for {name}:\")\n",
        "    print(df.head(), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eon4eMZ-DoTX"
      },
      "source": [
        "# Step 2: Tokenization, Stopword Removal, and NLP Processing\n",
        "\n",
        "This section focuses on preparing the text for Natural Language Processing (NLP) by:\n",
        "\n",
        "1. Tokenizing the text into words.\n",
        "2. Removing irrelevant stopwords for efficiency.\n",
        "3. Applying additional NLP tasks like stemming, lemmatization, and part-of-speech (POS) tagging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJvhBiNvDdDa",
        "outputId": "fce1a9fc-dcb8-473f-b4c2-5ea896948945"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Download necessary NLTK data files\n",
        "nltk.download('punkt')  # Download the 'punkt' package, used by NLTK for tokenizing sentences\n",
        "nltk.download('stopwords')  # Download the 'stopwords' package, which contains lists of stop words for several languages\n",
        "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
        "nltk.download('wordnet')  # For lemmatization\n",
        "nltk.download('punkt')  # For tokenizing (if not already downloaded)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Define English stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aZOUtXNWEK69"
      },
      "outputs": [],
      "source": [
        "def clean_and_tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text, removes stopwords, and cleans it, including correcting specific encoding issues.\n",
        "    \"\"\"\n",
        "    # Ensure input is a string\n",
        "    text = str(text)\n",
        "    # Correct specific encoding issues by removing problematic characters\n",
        "    text = re.sub(r'[â]', '', text)\n",
        "    # Remove special characters and extra spaces\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower().strip()\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return filtered_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBteFxEbHk14",
        "outputId": "54504d3b-7295-4035-b6e1-e5c7ae23ec7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['test', 'sentence', 'punctuation', 'common', 'words']\n"
          ]
        }
      ],
      "source": [
        "# Now, let's test the function with a sample text\n",
        "\n",
        "test_text = \"This is a test sentence with punctuation! And some common words.\"\n",
        "result = clean_and_tokenize(test_text)\n",
        "print(result)  # Expected to print the list of words after processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHEbAh9xFiUj",
        "outputId": "592b8db2-8af4-408b-b68d-01d8a5125ee7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing column 'sentence' in dataset:\n",
            "    character                                         sentence\n",
            "0  dumbledore  [shouldve, known, would, professor, mcgonagall]\n",
            "1  mcgonagall           [good, evening, professor, dumbledore]\n",
            "2  mcgonagall                            [rumors, true, albus]\n",
            "3  dumbledore                          [im, afraid, professor]\n",
            "4  dumbledore                                      [good, bad] \n",
            "\n",
            "Processing column 'sentence' in dataset:\n",
            "  character                                    sentence\n",
            "0     harry                         [cant, let, hedwig]\n",
            "1     harry  [im, allowed, use, magic, outside, school]\n",
            "2     harry                    [besides, uncle, vernon]\n",
            "3    vernon                             [harry, potter]\n",
            "4     harry                               [youve, done] \n",
            "\n",
            "Processing column 'sentence' in dataset:\n",
            "      character         sentence\n",
            "0         harry  [lumos, maxima]\n",
            "1         harry  [lumos, maxima]\n",
            "2         harry  [lumos, maxima]\n",
            "3         harry  [lumos, maxima]\n",
            "4  aunt petunia   [harry, harry] \n",
            "\n",
            "Processing column 'effect' in dataset:\n",
            "                      name                                  known_ingredients  \\\n",
            "0            ageing potion                             newt spleens , bananas   \n",
            "1               amortentia                                            unknown   \n",
            "2  antidote to veritaserum                                            unknown   \n",
            "3        babbling beverage                  valerian sprigs, aconite, dittany   \n",
            "4  baruffio's brain elixir  leaping toadstools, frog brains, runespoor egg...   \n",
            "\n",
            "                                              effect  \\\n",
            "0                       [ages, drinker, temporarily]   \n",
            "1  [love, potion, causes, powerful, infatuation, ...   \n",
            "2                    [counters, effect, veritaserum]   \n",
            "3                 [causes, drinker, speak, nonsense]   \n",
            "4         [allegedly, increases, ones, brain, power]   \n",
            "\n",
            "                                     characteristics difficulty_level  \n",
            "0                                              green  advanced,,,,,,,  \n",
            "1  mother-of-pearl sheen, spiralling steam, scent...   advanced,,,,,,  \n",
            "2                                            unknown         ,,,,,,,,  \n",
            "3                                            unknown           ,,,,,,  \n",
            "4                                    green in colour            ,,,,,   \n",
            "\n",
            "Processing column 'effect' in dataset:\n",
            "                                name       incantation                type  \\\n",
            "0                    summoning charm             accio               charm   \n",
            "1                           age line           unknown               charm   \n",
            "2                 water-making spell         aguamenti  charm, conjuration   \n",
            "3   launch an object up into the air  alarte ascendare               charm   \n",
            "4  albus dumbledore's forceful spell           unknown               spell   \n",
            "\n",
            "                                             effect     light  \n",
            "0                                 [summons, object]   unknown  \n",
            "1  [prevents, people, certain, age, access, target]      blue  \n",
            "2                                 [conjures, water]  icy blue  \n",
            "3                         [rockets, target, upward]       red  \n",
            "4                                    [great, force]   unknown   \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Now apply the cleaning and tokenization function to relevant text columns\n",
        "\n",
        "for df, text_column in zip(\n",
        "    [df1, df2, df3, df4, df5],\n",
        "    ['sentence', 'sentence', 'sentence', 'effect', 'effect']\n",
        "):\n",
        "    if text_column in df.columns:\n",
        "        print(f\"Processing column '{text_column}' in dataset:\")\n",
        "        df[text_column] = df[text_column].apply(clean_and_tokenize)\n",
        "        print(df.head(), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6bvCgcaCIvin"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"Converts treebank POS (Part-of-speech) tags to wordnet POS tags -> for lemming\"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def clean_and_tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text, removes stopwords, and cleans it, including correcting specific encoding issues.\n",
        "    \"\"\"\n",
        "    # Ensure input is a string\n",
        "    text = str(text)\n",
        "    # Correct specific encoding issues by removing problematic characters\n",
        "    text = re.sub(r'[â]', '', text)\n",
        "    # Remove special characters and extra spaces\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower().strip()\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Get POS tags\n",
        "    pos_tags = pos_tag(filtered_tokens)\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stems = [stemmer.stem(token) for token in filtered_tokens]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(tag) or wordnet.NOUN) for token, tag in pos_tags]\n",
        "\n",
        "    # Convert verbs to past tense (rudimentary approach)\n",
        "    past_tense_verbs = [lemmatizer.lemmatize(token, 'v') for token in filtered_tokens]\n",
        "\n",
        "    return tokens, pos_tags, stems, lemmas, past_tense_verbs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaY5S2EXKq-N"
      },
      "source": [
        "# Step 3: Structuring the Corpus for Indexing\n",
        "\n",
        "This step organizes the datasets into a structured corpus suitable for indexing and query-based retrieval. Sentences are grouped by relevant attributes (e.g., characters, chapters) to create meaningful document units.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oDTxmIqGKGF0"
      },
      "outputs": [],
      "source": [
        "# Function to structure data into a corpus\n",
        "def structure_corpus(df, group_by_column, text_column):\n",
        "    \"\"\"\n",
        "    Groups text data by a specified column to create a structured corpus.\n",
        "    - group_by_column: Attribute to group the data (e.g., 'character').\n",
        "    - text_column: Column containing text to aggregate.\n",
        "    Returns a dictionary where keys are group values and values are aggregated text.\n",
        "    \"\"\"\n",
        "    corpus = defaultdict(list)\n",
        "    for key, group in df.groupby(group_by_column):\n",
        "        document_text = \" \".join([\" \".join(sentence) for sentence in group[text_column] if sentence])  # Join tokens\n",
        "        corpus[key].append(document_text)\n",
        "    return dict(corpus)  # Convert defaultdict to a regular dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r74KsqDK5dt",
        "outputId": "a8d24943-99c8-4861-9a05-92ffaba56f54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Structured corpus saved to /content/structured_corpus.json\n",
            "Sample of structured corpus:\n",
            "all: ['ahhhhhhh yay']...\n",
            "\n",
            "all 3: ['know sorcerers stone']...\n",
            "\n",
            "barkeepâ tom: ['ah hagrid usual presume bless soul harry potter']...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example: Structuring df1 into a corpus grouped by 'character'\n",
        "# Creates structured_corpus.json\n",
        "try:\n",
        "    structured_corpus = structure_corpus(df1, group_by_column='character', text_column='sentence')\n",
        "\n",
        "    # Save the structured corpus as a JSON file\n",
        "    output_file = \"/content/structured_corpus.json\"\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(structured_corpus, f, indent=4)\n",
        "    print(f\"Structured corpus saved to {output_file}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error while structuring the corpus: {e}\")\n",
        "\n",
        "\n",
        "# Display a sample of the structured corpus\n",
        "print(\"Sample of structured corpus:\")\n",
        "for key, value in list(structured_corpus.items())[:3]:  # Display the first 3 entries\n",
        "    print(f\"{key}: {value[:100]}...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX2U-HjdM3zq",
        "outputId": "c77e9bbe-8f6e-44f3-c991-a3444c6b7181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Structured corpus saved to /content/structured_corpus_df2.json\n",
            "Sample of structured corpus:\n",
            "aragog:  hagrid  yes hagrid never sent men hollow thats lie hagrid never opened chamber secrets  monster bor...\n",
            "\n",
            "aunt petunia: pupkins masons arrive lounge waiting welcome graciously home oh...\n",
            "\n",
            "auntâ petuniaâ & dudley: aaah...\n",
            "\n",
            "Structured corpus saved to /content/structured_corpus_df3.json\n",
            "Sample of structured corpus:\n",
            "aunt marge: still dont say yes ungrateful way damn good brother keep hed straight orphanage hed dumped doorstep ...\n",
            "\n",
            "aunt petunia: harry harry harry open door marge lovely see nothing didnt work unemployed oh vernon...\n",
            "\n",
            "bem: grin idiot grim taking form giant spectral dog among darkest omens world omen death get fly thats ri...\n",
            "\n",
            "Structured corpus saved to /content/structured_corpus_df4.json\n",
            "Sample of structured corpus:\n",
            "after addition of final ingredient: taste and colour vary depending on the person being turned into\";advanced,,,,,,,,: unknown...\n",
            "\n",
            "ageing potion: ages drinker temporarily...\n",
            "\n",
            "amortentia: love potion causes powerful infatuation obsession drinker...\n",
            "\n",
            "Structured corpus saved to /content/structured_corpus_df5.json\n",
            "Sample of structured corpus:\n",
            "age line: prevents people certain age access target...\n",
            "\n",
            "albus dumbledore's forceful spell: great force...\n",
            "\n",
            "amplifying charm: loudens target...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Creates files strucred_corpus_df2-5 BUT the column names has to match!\n",
        "\n",
        "def structure_corpus(df, group_by_column, text_column):\n",
        "    # Check and convert any list in the text column to a single string\n",
        "    df[text_column] = df[text_column].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
        "\n",
        "    # Group by the specified column and concatenate text in the same group\n",
        "    grouped_data = df.groupby(group_by_column)[text_column].apply(' '.join).to_dict()\n",
        "    return grouped_data\n",
        "\n",
        "# Adjust the columns for df4 and df5 according to their structure\n",
        "dfs = [\n",
        "    (df2, 'character', 'sentence', \"/content/structured_corpus_df2.json\"),\n",
        "    (df3, 'character', 'sentence', \"/content/structured_corpus_df3.json\"),\n",
        "    (df4, 'name', 'effect', \"/content/structured_corpus_df4.json\"),  # Assume 'name' is the grouping column\n",
        "    (df5, 'name', 'effect', \"/content/structured_corpus_df5.json\")   # Same as above for df5\n",
        "]\n",
        "\n",
        "# Process each DataFrame\n",
        "for df, group_by, text_col, output_path in dfs:\n",
        "    try:\n",
        "        structured_corpus = structure_corpus(df, group_by_column=group_by, text_column=text_col)\n",
        "\n",
        "        # Save the structured corpus as a JSON file\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(structured_corpus, f, indent=4)\n",
        "        print(f\"Structured corpus saved to {output_path}\")\n",
        "\n",
        "        # Display a sample of the structured corpus\n",
        "        print(\"Sample of structured corpus:\")\n",
        "        for key, value in list(structured_corpus.items())[:3]:  # Display the first 3 entries\n",
        "            print(f\"{key}: {value[:100]}...\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error while structuring the corpus for {output_path}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIeTDfzkLGdP"
      },
      "source": [
        "# Step 4: Inverted Index Construction\n",
        "This step constructs an inverted index, a key data structure for information retrieval. The inverted index maps unique terms to the list of document IDs where they occur, enabling efficient query processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ICqKan8CK-v1"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Tokenization and Normalization Function\n",
        "def tokenize_and_normalize(text):\n",
        "    \"\"\"\n",
        "    Tokenizes and normalizes input text.\n",
        "    - Converts to lowercase.\n",
        "    - Removes non-alphanumeric characters.\n",
        "    Returns a list of tokens.\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-pZXFBOZLrh3"
      },
      "outputs": [],
      "source": [
        "# Inverted Index Construction\n",
        "def build_inverted_index(corpus):\n",
        "    \"\"\"\n",
        "    Builds an inverted index from the input corpus.\n",
        "    - corpus: Dictionary where keys are document IDs and values are document text.\n",
        "    Returns a dictionary where:\n",
        "        - Keys are unique terms.\n",
        "        - Values are lists of document IDs containing the term.\n",
        "    \"\"\"\n",
        "    inverted_index = defaultdict(set)  # Use a set to avoid duplicate document IDs\n",
        "    for doc_id, content in corpus.items():\n",
        "        tokens = tokenize_and_normalize(content)\n",
        "        for token in tokens:\n",
        "            inverted_index[token].add(doc_id)\n",
        "\n",
        "    # Convert sets to lists for easier JSON serialization\n",
        "    return {term: list(doc_ids) for term, doc_ids in inverted_index.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMxXYnlUNgVC",
        "outputId": "152952f4-1af8-40fd-9177-ff58f7220a30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inverted index saved to /content/inverted_index.json\n",
            "Sample of inverted index:\n",
            "prevents: ['anti-disapparition jinx', 'anti-cheating spell', 'age line']\n",
            "people: ['levicorpus', 'age line']\n",
            "certain: ['anti-disapparition jinx', 'age line', 'ministry of magic fog']\n",
            "age: ['age line']\n",
            "access: ['age line']\n"
          ]
        }
      ],
      "source": [
        "# Build the Inverted Index (json)\n",
        "try:\n",
        "    inverted_index = build_inverted_index(structured_corpus)\n",
        "\n",
        "    # Save the Inverted Index to a JSON File\n",
        "    output_file = \"/content/inverted_index.json\"\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(inverted_index, f, indent=4)\n",
        "    print(f\"Inverted index saved to {output_file}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error while building the inverted index: {e}\")\n",
        "\n",
        "# Display a sample of the inverted index\n",
        "print(\"Sample of inverted index:\")\n",
        "for term, doc_ids in list(inverted_index.items())[:5]:  # Display the first 5 terms\n",
        "    print(f\"{term}: {doc_ids}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpLtIisfNuqZ"
      },
      "source": [
        "# Step 5: Query Processing with TF-IDF and Cosine Similarity\n",
        "\n",
        "This step processes user queries to retrieve and rank relevant documents. It leverages the Term Frequency-Inverse Document Frequency (TF-IDF) method to compute the importance of terms and cosine similarity to rank documents based on relevance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mEUprQdfN7vY"
      },
      "outputs": [],
      "source": [
        "# Function to load documents from JSON files\n",
        "def load_documents(file_paths):\n",
        "    documents = {}  # Initialize an empty dictionary to store the documents\n",
        "\n",
        "    # Iterate over each file path provided in the list\n",
        "    for path in file_paths:\n",
        "        with open(path, 'r') as file:\n",
        "            data = json.load(file)  # Load the JSON content of the file into a Python dictionary\n",
        "\n",
        "            # Iterate over each document ID and its content in the JSON data\n",
        "            for doc_id, content in data.items():\n",
        "                # Check if the content is a list (handling cases where the content might be a list of strings)\n",
        "                if isinstance(content, list):\n",
        "                    # Join list elements into a single string separated by spaces\n",
        "                    content = ' '.join(content)\n",
        "\n",
        "                # Add the document ID and its corresponding content (now ensured to be a string) to the documents dictionary\n",
        "                documents[doc_id] = content\n",
        "\n",
        "    return documents  # Return the dictionary containing all documents loaded and processed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "4dYN-zpMPAF7"
      },
      "outputs": [],
      "source": [
        "# Paths to the JSON files\n",
        "file_paths = [\n",
        "    \"/content/structured_corpus.json\",\n",
        "    \"/content/structured_corpus_df2.json\",\n",
        "    \"/content/structured_corpus_df3.json\",\n",
        "    \"/content/structured_corpus_df4.json\",\n",
        "    \"/content/structured_corpus_df5.json\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "UferKI3qPC3N"
      },
      "outputs": [],
      "source": [
        "# Load documents\n",
        "documents = load_documents(file_paths)\n",
        "query = \"Harry and Hermione\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA3pThXoPm47"
      },
      "source": [
        "# Computing TF-IDF Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCAK8F92UVkX"
      },
      "source": [
        "Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus of documents. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus\n",
        "\n",
        "- Purpose of TF-IDF\n",
        "Relevance: TF-IDF is a way to score the importance of words (or \"terms\") in a document based on how frequently they appear across multiple documents. If a word appears frequently in a document but not across many documents, it is likely to be important. Conversely, if a word appears in many documents, it's not a unique identifier.\n",
        "\n",
        "- Feature Representation: For many tasks in Natural Language Processing (NLP), such as search engines and information retrieval, document classification, and topic modeling, having a numerical representation of text data is crucial. TF-IDF transforms the text into a usable vector format, where each word is represented by a score that signifies its relevance in the context of a given document within a larger corpus.\n",
        "\n",
        "### Process of Computing TF-IDF\n",
        "- TF (Term Frequency): Measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (the total number of terms in the document) as a way of normalization.\n",
        "\n",
        "- IDF (Inverse Document Frequency): Measures how important a term is within the entire corpus. It is calculated by dividing the total number of documents by the number of documents containing the term, and the logarithm of this quotient is then taken.\n",
        "\n",
        "### Usage in Vectorization\n",
        "- Vectorization: The combined use of TF and IDF represents a balanced way of understanding not only the local importance of a term (in terms of its frequency within a particular document) but also its global importance (in terms of its frequency across all documents in the corpus). This makes TF-IDF a very popular and powerful feature extraction tool to facilitate various machine learning algorithms dealing with text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7xxbGqoVPHtO"
      },
      "outputs": [],
      "source": [
        "# Function to compute TF-IDF vectors\n",
        "def compute_tfidf(documents, query):\n",
        "    \"\"\"\n",
        "    Computes TF-IDF vectors for the given documents and query.\n",
        "    - documents: Dictionary of {doc_id: content}.\n",
        "    - query: User query string.\n",
        "    Returns:\n",
        "        - Document vectors (TF-IDF).\n",
        "        - Query vector (TF-IDF).\n",
        "        - List of feature names (terms).\n",
        "    \"\"\"\n",
        "    # Combine all document texts and the query into a single list for vectorization\n",
        "    all_texts = list(documents.values()) + [query]\n",
        "\n",
        "    # Initialize a TfidfVectorizer object to transform text into a vector of term frequency-inverse document frequency features\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    # Fit the vectorizer to the combined texts and transform the texts into a TF-IDF matrix\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "    # Retrieve the list of terms used in the TF-IDF matrix, corresponding to the columns in the matrix\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Extract the vectors for all documents (all rows except the last one) from the TF-IDF matrix\n",
        "    doc_vectors = tfidf_matrix[:-1]  # All but the last row\n",
        "    # Extract the vector for the query (the last row of the TF-IDF matrix)\n",
        "    query_vector = tfidf_matrix[-1]  # The last row\n",
        "\n",
        "    return doc_vectors, query_vector, feature_names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM9iu1SUP0R1"
      },
      "source": [
        "# Computing Cosine similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbCc_n9yVFgR"
      },
      "source": [
        "Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. This measure is widely used in text analysis to quantify the similarity between two texts as represented by their term vector representations, especially in systems involving matching documents, such as information retrieval and machine learning.\n",
        "\n",
        "**Key Points on Cosine Similarity in Document Ranking**\n",
        "- Independence from Size: Cosine similarity focuses on the orientation of vectors, not their magnitude. This characteristic makes it useful for text data where the lengths of documents may vary significantly.\n",
        "\n",
        "- Application in Information Retrieval: In the context of TF-IDF vectors, cosine similarity is used to determine how similar a document is to a given query. This approach is particularly beneficial in search engines and document retrieval systems, where the goal is to find documents that are \"closest\" to a search query.\n",
        "\n",
        "- Effective for Sparse Vectors: Given that TF-IDF often results in sparse vectors (many elements are zero), cosine similarity is a robust measure because it inherently deals with non-zero dimensions of the vectors, emphasizing the presence and frequency of terms over the absence of terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mAMtlIXrPtZL"
      },
      "outputs": [],
      "source": [
        "# Function to rank documents based on cosine similarity\n",
        "def rank_by_cosine_similarity(doc_vectors, query_vector):\n",
        "    \"\"\"\n",
        "    Ranks documents based on cosine similarity to the query.\n",
        "    - doc_vectors: TF-IDF vectors of the documents.\n",
        "    - query_vector: TF-IDF vector of the query.\n",
        "    Returns a sorted list of tuples (doc_index, similarity_score).\n",
        "    \"\"\"\n",
        "    # Calculate cosine similarity between the query vector and each document vector.\n",
        "    # The result is a 1D array of similarity scores.\n",
        "    similarities = cosine_similarity(query_vector, doc_vectors)[0]\n",
        "\n",
        "    # Create a list of tuples (document index, similarity score) and sort them.\n",
        "    # The sorting is based on the similarity score, in descending order.\n",
        "    ranked_docs = sorted(\n",
        "        enumerate(similarities, 1),  # Enumerate starting at index 1 for document numbering\n",
        "        key=lambda x: x[1],          # Sort by the similarity score, which is the second item in the tuple\n",
        "        reverse=True                 # Sort in descending order to put the most similar documents first\n",
        "    )\n",
        "\n",
        "    return ranked_docs  # Return the sorted list of documents based on their similarity to the query\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br_2w4PwP5SI",
        "outputId": "ab8ec9aa-30ee-440d-9da1-d92372df87c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Cosine Similarity Ranking:\n",
            "Rank 1: Document 42 (Score: 0.4524)\n",
            "Rank 2: Document 53 (Score: 0.2658)\n",
            "Rank 3: Document 89 (Score: 0.2603)\n",
            "Rank 4: Document 35 (Score: 0.2093)\n",
            "Rank 5: Document 57 (Score: 0.2020)\n",
            "Rank 6: Document 25 (Score: 0.1997)\n",
            "Rank 7: Document 76 (Score: 0.1910)\n",
            "Rank 8: Document 41 (Score: 0.1556)\n",
            "Rank 9: Document 59 (Score: 0.1444)\n",
            "Rank 10: Document 44 (Score: 0.1413)\n",
            "Rank 11: Document 20 (Score: 0.1386)\n",
            "Rank 12: Document 94 (Score: 0.1152)\n",
            "Rank 13: Document 71 (Score: 0.1021)\n",
            "Rank 14: Document 3 (Score: 0.0928)\n",
            "Rank 15: Document 63 (Score: 0.0830)\n",
            "Rank 16: Document 52 (Score: 0.0804)\n",
            "Rank 17: Document 67 (Score: 0.0788)\n",
            "Rank 18: Document 24 (Score: 0.0766)\n",
            "Rank 19: Document 23 (Score: 0.0738)\n",
            "Rank 20: Document 85 (Score: 0.0706)\n",
            "Rank 21: Document 51 (Score: 0.0673)\n",
            "Rank 22: Document 62 (Score: 0.0672)\n",
            "Rank 23: Document 16 (Score: 0.0667)\n",
            "Rank 24: Document 27 (Score: 0.0650)\n",
            "Rank 25: Document 13 (Score: 0.0641)\n",
            "Rank 26: Document 78 (Score: 0.0628)\n",
            "Rank 27: Document 33 (Score: 0.0544)\n",
            "Rank 28: Document 34 (Score: 0.0475)\n",
            "Rank 29: Document 66 (Score: 0.0471)\n",
            "Rank 30: Document 43 (Score: 0.0464)\n",
            "Rank 31: Document 74 (Score: 0.0423)\n",
            "Rank 32: Document 32 (Score: 0.0397)\n",
            "Rank 33: Document 95 (Score: 0.0379)\n",
            "Rank 34: Document 102 (Score: 0.0363)\n",
            "Rank 35: Document 17 (Score: 0.0327)\n",
            "Rank 36: Document 83 (Score: 0.0327)\n",
            "Rank 37: Document 98 (Score: 0.0301)\n",
            "Rank 38: Document 86 (Score: 0.0291)\n",
            "Rank 39: Document 29 (Score: 0.0225)\n",
            "Rank 40: Document 40 (Score: 0.0195)\n",
            "Rank 41: Document 1 (Score: 0.0000)\n",
            "Rank 42: Document 2 (Score: 0.0000)\n",
            "Rank 43: Document 4 (Score: 0.0000)\n",
            "Rank 44: Document 5 (Score: 0.0000)\n",
            "Rank 45: Document 6 (Score: 0.0000)\n",
            "Rank 46: Document 7 (Score: 0.0000)\n",
            "Rank 47: Document 8 (Score: 0.0000)\n",
            "Rank 48: Document 9 (Score: 0.0000)\n",
            "Rank 49: Document 10 (Score: 0.0000)\n",
            "Rank 50: Document 11 (Score: 0.0000)\n",
            "Rank 51: Document 12 (Score: 0.0000)\n",
            "Rank 52: Document 14 (Score: 0.0000)\n",
            "Rank 53: Document 15 (Score: 0.0000)\n",
            "Rank 54: Document 18 (Score: 0.0000)\n",
            "Rank 55: Document 19 (Score: 0.0000)\n",
            "Rank 56: Document 21 (Score: 0.0000)\n",
            "Rank 57: Document 22 (Score: 0.0000)\n",
            "Rank 58: Document 26 (Score: 0.0000)\n",
            "Rank 59: Document 28 (Score: 0.0000)\n",
            "Rank 60: Document 30 (Score: 0.0000)\n",
            "Rank 61: Document 31 (Score: 0.0000)\n",
            "Rank 62: Document 36 (Score: 0.0000)\n",
            "Rank 63: Document 37 (Score: 0.0000)\n",
            "Rank 64: Document 38 (Score: 0.0000)\n",
            "Rank 65: Document 39 (Score: 0.0000)\n",
            "Rank 66: Document 45 (Score: 0.0000)\n",
            "Rank 67: Document 46 (Score: 0.0000)\n",
            "Rank 68: Document 47 (Score: 0.0000)\n",
            "Rank 69: Document 48 (Score: 0.0000)\n",
            "Rank 70: Document 49 (Score: 0.0000)\n",
            "Rank 71: Document 50 (Score: 0.0000)\n",
            "Rank 72: Document 54 (Score: 0.0000)\n",
            "Rank 73: Document 55 (Score: 0.0000)\n",
            "Rank 74: Document 56 (Score: 0.0000)\n",
            "Rank 75: Document 58 (Score: 0.0000)\n",
            "Rank 76: Document 60 (Score: 0.0000)\n",
            "Rank 77: Document 61 (Score: 0.0000)\n",
            "Rank 78: Document 64 (Score: 0.0000)\n",
            "Rank 79: Document 65 (Score: 0.0000)\n",
            "Rank 80: Document 68 (Score: 0.0000)\n",
            "Rank 81: Document 69 (Score: 0.0000)\n",
            "Rank 82: Document 70 (Score: 0.0000)\n",
            "Rank 83: Document 72 (Score: 0.0000)\n",
            "Rank 84: Document 73 (Score: 0.0000)\n",
            "Rank 85: Document 75 (Score: 0.0000)\n",
            "Rank 86: Document 77 (Score: 0.0000)\n",
            "Rank 87: Document 79 (Score: 0.0000)\n",
            "Rank 88: Document 80 (Score: 0.0000)\n",
            "Rank 89: Document 81 (Score: 0.0000)\n",
            "Rank 90: Document 82 (Score: 0.0000)\n",
            "Rank 91: Document 84 (Score: 0.0000)\n",
            "Rank 92: Document 87 (Score: 0.0000)\n",
            "Rank 93: Document 88 (Score: 0.0000)\n",
            "Rank 94: Document 90 (Score: 0.0000)\n",
            "Rank 95: Document 91 (Score: 0.0000)\n",
            "Rank 96: Document 92 (Score: 0.0000)\n",
            "Rank 97: Document 93 (Score: 0.0000)\n",
            "Rank 98: Document 96 (Score: 0.0000)\n",
            "Rank 99: Document 97 (Score: 0.0000)\n",
            "Rank 100: Document 99 (Score: 0.0000)\n",
            "Rank 101: Document 100 (Score: 0.0000)\n",
            "Rank 102: Document 101 (Score: 0.0000)\n",
            "Rank 103: Document 103 (Score: 0.0000)\n",
            "Rank 104: Document 104 (Score: 0.0000)\n",
            "Rank 105: Document 105 (Score: 0.0000)\n",
            "Rank 106: Document 106 (Score: 0.0000)\n",
            "Rank 107: Document 107 (Score: 0.0000)\n",
            "Rank 108: Document 108 (Score: 0.0000)\n",
            "Rank 109: Document 109 (Score: 0.0000)\n",
            "Rank 110: Document 110 (Score: 0.0000)\n",
            "Rank 111: Document 111 (Score: 0.0000)\n",
            "Rank 112: Document 112 (Score: 0.0000)\n",
            "Rank 113: Document 113 (Score: 0.0000)\n",
            "Rank 114: Document 114 (Score: 0.0000)\n",
            "Rank 115: Document 115 (Score: 0.0000)\n",
            "Rank 116: Document 116 (Score: 0.0000)\n",
            "Rank 117: Document 117 (Score: 0.0000)\n",
            "Rank 118: Document 118 (Score: 0.0000)\n",
            "Rank 119: Document 119 (Score: 0.0000)\n",
            "Rank 120: Document 120 (Score: 0.0000)\n",
            "Rank 121: Document 121 (Score: 0.0000)\n",
            "Rank 122: Document 122 (Score: 0.0000)\n",
            "Rank 123: Document 123 (Score: 0.0000)\n",
            "Rank 124: Document 124 (Score: 0.0000)\n",
            "Rank 125: Document 125 (Score: 0.0000)\n",
            "Rank 126: Document 126 (Score: 0.0000)\n",
            "Rank 127: Document 127 (Score: 0.0000)\n",
            "Rank 128: Document 128 (Score: 0.0000)\n",
            "Rank 129: Document 129 (Score: 0.0000)\n",
            "Rank 130: Document 130 (Score: 0.0000)\n",
            "Rank 131: Document 131 (Score: 0.0000)\n",
            "Rank 132: Document 132 (Score: 0.0000)\n",
            "Rank 133: Document 133 (Score: 0.0000)\n",
            "Rank 134: Document 134 (Score: 0.0000)\n",
            "Rank 135: Document 135 (Score: 0.0000)\n",
            "Rank 136: Document 136 (Score: 0.0000)\n",
            "Rank 137: Document 137 (Score: 0.0000)\n",
            "Rank 138: Document 138 (Score: 0.0000)\n",
            "Rank 139: Document 139 (Score: 0.0000)\n",
            "Rank 140: Document 140 (Score: 0.0000)\n",
            "Rank 141: Document 141 (Score: 0.0000)\n",
            "Rank 142: Document 142 (Score: 0.0000)\n",
            "Rank 143: Document 143 (Score: 0.0000)\n",
            "Rank 144: Document 144 (Score: 0.0000)\n",
            "Rank 145: Document 145 (Score: 0.0000)\n",
            "Rank 146: Document 146 (Score: 0.0000)\n",
            "Rank 147: Document 147 (Score: 0.0000)\n",
            "Rank 148: Document 148 (Score: 0.0000)\n",
            "Rank 149: Document 149 (Score: 0.0000)\n",
            "Rank 150: Document 150 (Score: 0.0000)\n",
            "Rank 151: Document 151 (Score: 0.0000)\n",
            "Rank 152: Document 152 (Score: 0.0000)\n",
            "Rank 153: Document 153 (Score: 0.0000)\n",
            "Rank 154: Document 154 (Score: 0.0000)\n",
            "Rank 155: Document 155 (Score: 0.0000)\n",
            "Rank 156: Document 156 (Score: 0.0000)\n",
            "Rank 157: Document 157 (Score: 0.0000)\n",
            "Rank 158: Document 158 (Score: 0.0000)\n",
            "Rank 159: Document 159 (Score: 0.0000)\n",
            "Rank 160: Document 160 (Score: 0.0000)\n",
            "Rank 161: Document 161 (Score: 0.0000)\n",
            "Rank 162: Document 162 (Score: 0.0000)\n",
            "Rank 163: Document 163 (Score: 0.0000)\n",
            "Rank 164: Document 164 (Score: 0.0000)\n",
            "Rank 165: Document 165 (Score: 0.0000)\n",
            "Rank 166: Document 166 (Score: 0.0000)\n",
            "Rank 167: Document 167 (Score: 0.0000)\n",
            "Rank 168: Document 168 (Score: 0.0000)\n",
            "Rank 169: Document 169 (Score: 0.0000)\n",
            "Rank 170: Document 170 (Score: 0.0000)\n",
            "Rank 171: Document 171 (Score: 0.0000)\n",
            "Rank 172: Document 172 (Score: 0.0000)\n",
            "Rank 173: Document 173 (Score: 0.0000)\n",
            "Rank 174: Document 174 (Score: 0.0000)\n",
            "Rank 175: Document 175 (Score: 0.0000)\n",
            "Rank 176: Document 176 (Score: 0.0000)\n",
            "Rank 177: Document 177 (Score: 0.0000)\n",
            "Rank 178: Document 178 (Score: 0.0000)\n",
            "Rank 179: Document 179 (Score: 0.0000)\n",
            "Rank 180: Document 180 (Score: 0.0000)\n",
            "Rank 181: Document 181 (Score: 0.0000)\n",
            "Rank 182: Document 182 (Score: 0.0000)\n",
            "Rank 183: Document 183 (Score: 0.0000)\n",
            "Rank 184: Document 184 (Score: 0.0000)\n",
            "Rank 185: Document 185 (Score: 0.0000)\n",
            "Rank 186: Document 186 (Score: 0.0000)\n",
            "Rank 187: Document 187 (Score: 0.0000)\n",
            "Rank 188: Document 188 (Score: 0.0000)\n",
            "Rank 189: Document 189 (Score: 0.0000)\n",
            "Rank 190: Document 190 (Score: 0.0000)\n",
            "Rank 191: Document 191 (Score: 0.0000)\n",
            "Rank 192: Document 192 (Score: 0.0000)\n",
            "Rank 193: Document 193 (Score: 0.0000)\n",
            "Rank 194: Document 194 (Score: 0.0000)\n",
            "Rank 195: Document 195 (Score: 0.0000)\n",
            "Rank 196: Document 196 (Score: 0.0000)\n",
            "Rank 197: Document 197 (Score: 0.0000)\n",
            "Rank 198: Document 198 (Score: 0.0000)\n",
            "Rank 199: Document 199 (Score: 0.0000)\n",
            "Rank 200: Document 200 (Score: 0.0000)\n",
            "Rank 201: Document 201 (Score: 0.0000)\n",
            "Rank 202: Document 202 (Score: 0.0000)\n",
            "Rank 203: Document 203 (Score: 0.0000)\n",
            "Rank 204: Document 204 (Score: 0.0000)\n",
            "Rank 205: Document 205 (Score: 0.0000)\n",
            "Rank 206: Document 206 (Score: 0.0000)\n",
            "Rank 207: Document 207 (Score: 0.0000)\n",
            "Rank 208: Document 208 (Score: 0.0000)\n",
            "Rank 209: Document 209 (Score: 0.0000)\n",
            "Rank 210: Document 210 (Score: 0.0000)\n",
            "Rank 211: Document 211 (Score: 0.0000)\n",
            "Rank 212: Document 212 (Score: 0.0000)\n",
            "Rank 213: Document 213 (Score: 0.0000)\n",
            "Rank 214: Document 214 (Score: 0.0000)\n",
            "Rank 215: Document 215 (Score: 0.0000)\n",
            "Rank 216: Document 216 (Score: 0.0000)\n",
            "Rank 217: Document 217 (Score: 0.0000)\n",
            "Rank 218: Document 218 (Score: 0.0000)\n",
            "Rank 219: Document 219 (Score: 0.0000)\n",
            "Rank 220: Document 220 (Score: 0.0000)\n",
            "Rank 221: Document 221 (Score: 0.0000)\n",
            "Rank 222: Document 222 (Score: 0.0000)\n",
            "Rank 223: Document 223 (Score: 0.0000)\n",
            "Rank 224: Document 224 (Score: 0.0000)\n",
            "Rank 225: Document 225 (Score: 0.0000)\n",
            "Rank 226: Document 226 (Score: 0.0000)\n",
            "Rank 227: Document 227 (Score: 0.0000)\n",
            "Rank 228: Document 228 (Score: 0.0000)\n",
            "Rank 229: Document 229 (Score: 0.0000)\n",
            "Rank 230: Document 230 (Score: 0.0000)\n",
            "Rank 231: Document 231 (Score: 0.0000)\n",
            "Rank 232: Document 232 (Score: 0.0000)\n",
            "Rank 233: Document 233 (Score: 0.0000)\n",
            "Rank 234: Document 234 (Score: 0.0000)\n",
            "Rank 235: Document 235 (Score: 0.0000)\n",
            "Rank 236: Document 236 (Score: 0.0000)\n",
            "Rank 237: Document 237 (Score: 0.0000)\n",
            "Rank 238: Document 238 (Score: 0.0000)\n",
            "Rank 239: Document 239 (Score: 0.0000)\n",
            "Rank 240: Document 240 (Score: 0.0000)\n",
            "Rank 241: Document 241 (Score: 0.0000)\n",
            "Rank 242: Document 242 (Score: 0.0000)\n",
            "Rank 243: Document 243 (Score: 0.0000)\n",
            "Rank 244: Document 244 (Score: 0.0000)\n",
            "Rank 245: Document 245 (Score: 0.0000)\n",
            "Rank 246: Document 246 (Score: 0.0000)\n",
            "Rank 247: Document 247 (Score: 0.0000)\n",
            "Rank 248: Document 248 (Score: 0.0000)\n",
            "Rank 249: Document 249 (Score: 0.0000)\n",
            "Rank 250: Document 250 (Score: 0.0000)\n",
            "Rank 251: Document 251 (Score: 0.0000)\n",
            "Rank 252: Document 252 (Score: 0.0000)\n",
            "Rank 253: Document 253 (Score: 0.0000)\n",
            "Rank 254: Document 254 (Score: 0.0000)\n",
            "Rank 255: Document 255 (Score: 0.0000)\n",
            "Rank 256: Document 256 (Score: 0.0000)\n",
            "Rank 257: Document 257 (Score: 0.0000)\n",
            "Rank 258: Document 258 (Score: 0.0000)\n",
            "Rank 259: Document 259 (Score: 0.0000)\n",
            "Rank 260: Document 260 (Score: 0.0000)\n",
            "Rank 261: Document 261 (Score: 0.0000)\n",
            "Rank 262: Document 262 (Score: 0.0000)\n",
            "Rank 263: Document 263 (Score: 0.0000)\n",
            "Rank 264: Document 264 (Score: 0.0000)\n",
            "Rank 265: Document 265 (Score: 0.0000)\n",
            "Rank 266: Document 266 (Score: 0.0000)\n",
            "Rank 267: Document 267 (Score: 0.0000)\n",
            "Rank 268: Document 268 (Score: 0.0000)\n",
            "Rank 269: Document 269 (Score: 0.0000)\n",
            "Rank 270: Document 270 (Score: 0.0000)\n",
            "Rank 271: Document 271 (Score: 0.0000)\n",
            "Rank 272: Document 272 (Score: 0.0000)\n",
            "Rank 273: Document 273 (Score: 0.0000)\n",
            "Rank 274: Document 274 (Score: 0.0000)\n",
            "Rank 275: Document 275 (Score: 0.0000)\n",
            "Rank 276: Document 276 (Score: 0.0000)\n",
            "Rank 277: Document 277 (Score: 0.0000)\n",
            "Rank 278: Document 278 (Score: 0.0000)\n",
            "Rank 279: Document 279 (Score: 0.0000)\n",
            "Rank 280: Document 280 (Score: 0.0000)\n",
            "Rank 281: Document 281 (Score: 0.0000)\n",
            "Rank 282: Document 282 (Score: 0.0000)\n",
            "Rank 283: Document 283 (Score: 0.0000)\n",
            "Rank 284: Document 284 (Score: 0.0000)\n",
            "Rank 285: Document 285 (Score: 0.0000)\n",
            "Rank 286: Document 286 (Score: 0.0000)\n",
            "Rank 287: Document 287 (Score: 0.0000)\n",
            "Rank 288: Document 288 (Score: 0.0000)\n",
            "Rank 289: Document 289 (Score: 0.0000)\n",
            "Rank 290: Document 290 (Score: 0.0000)\n",
            "Rank 291: Document 291 (Score: 0.0000)\n",
            "Rank 292: Document 292 (Score: 0.0000)\n",
            "Rank 293: Document 293 (Score: 0.0000)\n",
            "Rank 294: Document 294 (Score: 0.0000)\n",
            "Rank 295: Document 295 (Score: 0.0000)\n",
            "Rank 296: Document 296 (Score: 0.0000)\n",
            "Rank 297: Document 297 (Score: 0.0000)\n",
            "Rank 298: Document 298 (Score: 0.0000)\n",
            "Rank 299: Document 299 (Score: 0.0000)\n",
            "Rank 300: Document 300 (Score: 0.0000)\n",
            "Rank 301: Document 301 (Score: 0.0000)\n",
            "Rank 302: Document 302 (Score: 0.0000)\n",
            "Rank 303: Document 303 (Score: 0.0000)\n",
            "Rank 304: Document 304 (Score: 0.0000)\n",
            "Rank 305: Document 305 (Score: 0.0000)\n",
            "Rank 306: Document 306 (Score: 0.0000)\n",
            "Rank 307: Document 307 (Score: 0.0000)\n",
            "Rank 308: Document 308 (Score: 0.0000)\n",
            "Rank 309: Document 309 (Score: 0.0000)\n",
            "Rank 310: Document 310 (Score: 0.0000)\n",
            "Rank 311: Document 311 (Score: 0.0000)\n",
            "Rank 312: Document 312 (Score: 0.0000)\n",
            "Rank 313: Document 313 (Score: 0.0000)\n",
            "Rank 314: Document 314 (Score: 0.0000)\n",
            "Rank 315: Document 315 (Score: 0.0000)\n",
            "Rank 316: Document 316 (Score: 0.0000)\n",
            "Rank 317: Document 317 (Score: 0.0000)\n",
            "Rank 318: Document 318 (Score: 0.0000)\n",
            "Rank 319: Document 319 (Score: 0.0000)\n",
            "Rank 320: Document 320 (Score: 0.0000)\n",
            "Rank 321: Document 321 (Score: 0.0000)\n",
            "Rank 322: Document 322 (Score: 0.0000)\n",
            "Rank 323: Document 323 (Score: 0.0000)\n",
            "Rank 324: Document 324 (Score: 0.0000)\n",
            "Rank 325: Document 325 (Score: 0.0000)\n",
            "Rank 326: Document 326 (Score: 0.0000)\n",
            "Rank 327: Document 327 (Score: 0.0000)\n",
            "Rank 328: Document 328 (Score: 0.0000)\n",
            "Rank 329: Document 329 (Score: 0.0000)\n",
            "Rank 330: Document 330 (Score: 0.0000)\n",
            "Rank 331: Document 331 (Score: 0.0000)\n",
            "Rank 332: Document 332 (Score: 0.0000)\n",
            "Rank 333: Document 333 (Score: 0.0000)\n",
            "Rank 334: Document 334 (Score: 0.0000)\n",
            "Rank 335: Document 335 (Score: 0.0000)\n",
            "Rank 336: Document 336 (Score: 0.0000)\n",
            "Rank 337: Document 337 (Score: 0.0000)\n",
            "Rank 338: Document 338 (Score: 0.0000)\n",
            "Rank 339: Document 339 (Score: 0.0000)\n",
            "Rank 340: Document 340 (Score: 0.0000)\n",
            "Rank 341: Document 341 (Score: 0.0000)\n",
            "Rank 342: Document 342 (Score: 0.0000)\n",
            "Rank 343: Document 343 (Score: 0.0000)\n",
            "Rank 344: Document 344 (Score: 0.0000)\n",
            "Rank 345: Document 345 (Score: 0.0000)\n",
            "Rank 346: Document 346 (Score: 0.0000)\n",
            "Rank 347: Document 347 (Score: 0.0000)\n",
            "Rank 348: Document 348 (Score: 0.0000)\n",
            "Rank 349: Document 349 (Score: 0.0000)\n",
            "Rank 350: Document 350 (Score: 0.0000)\n",
            "Rank 351: Document 351 (Score: 0.0000)\n",
            "Rank 352: Document 352 (Score: 0.0000)\n",
            "Rank 353: Document 353 (Score: 0.0000)\n",
            "Rank 354: Document 354 (Score: 0.0000)\n",
            "Rank 355: Document 355 (Score: 0.0000)\n",
            "Rank 356: Document 356 (Score: 0.0000)\n",
            "Rank 357: Document 357 (Score: 0.0000)\n",
            "Rank 358: Document 358 (Score: 0.0000)\n",
            "Rank 359: Document 359 (Score: 0.0000)\n",
            "Rank 360: Document 360 (Score: 0.0000)\n",
            "Rank 361: Document 361 (Score: 0.0000)\n",
            "Rank 362: Document 362 (Score: 0.0000)\n",
            "Rank 363: Document 363 (Score: 0.0000)\n",
            "Rank 364: Document 364 (Score: 0.0000)\n",
            "Rank 365: Document 365 (Score: 0.0000)\n",
            "Rank 366: Document 366 (Score: 0.0000)\n",
            "Rank 367: Document 367 (Score: 0.0000)\n",
            "Rank 368: Document 368 (Score: 0.0000)\n",
            "Rank 369: Document 369 (Score: 0.0000)\n",
            "Rank 370: Document 370 (Score: 0.0000)\n",
            "Rank 371: Document 371 (Score: 0.0000)\n",
            "Rank 372: Document 372 (Score: 0.0000)\n",
            "Rank 373: Document 373 (Score: 0.0000)\n",
            "Rank 374: Document 374 (Score: 0.0000)\n",
            "Rank 375: Document 375 (Score: 0.0000)\n",
            "Rank 376: Document 376 (Score: 0.0000)\n",
            "Rank 377: Document 377 (Score: 0.0000)\n",
            "Rank 378: Document 378 (Score: 0.0000)\n",
            "Rank 379: Document 379 (Score: 0.0000)\n",
            "Rank 380: Document 380 (Score: 0.0000)\n",
            "Rank 381: Document 381 (Score: 0.0000)\n",
            "Rank 382: Document 382 (Score: 0.0000)\n",
            "Rank 383: Document 383 (Score: 0.0000)\n",
            "Rank 384: Document 384 (Score: 0.0000)\n",
            "Rank 385: Document 385 (Score: 0.0000)\n",
            "Rank 386: Document 386 (Score: 0.0000)\n",
            "Rank 387: Document 387 (Score: 0.0000)\n",
            "Rank 388: Document 388 (Score: 0.0000)\n",
            "Rank 389: Document 389 (Score: 0.0000)\n",
            "Rank 390: Document 390 (Score: 0.0000)\n",
            "Rank 391: Document 391 (Score: 0.0000)\n",
            "Rank 392: Document 392 (Score: 0.0000)\n",
            "Rank 393: Document 393 (Score: 0.0000)\n",
            "Rank 394: Document 394 (Score: 0.0000)\n",
            "Rank 395: Document 395 (Score: 0.0000)\n",
            "Rank 396: Document 396 (Score: 0.0000)\n",
            "Rank 397: Document 397 (Score: 0.0000)\n",
            "Rank 398: Document 398 (Score: 0.0000)\n",
            "Rank 399: Document 399 (Score: 0.0000)\n",
            "Rank 400: Document 400 (Score: 0.0000)\n",
            "Rank 401: Document 401 (Score: 0.0000)\n",
            "Rank 402: Document 402 (Score: 0.0000)\n",
            "Rank 403: Document 403 (Score: 0.0000)\n",
            "Rank 404: Document 404 (Score: 0.0000)\n",
            "Rank 405: Document 405 (Score: 0.0000)\n",
            "Rank 406: Document 406 (Score: 0.0000)\n",
            "Rank 407: Document 407 (Score: 0.0000)\n",
            "Rank 408: Document 408 (Score: 0.0000)\n",
            "Rank 409: Document 409 (Score: 0.0000)\n",
            "Rank 410: Document 410 (Score: 0.0000)\n",
            "Rank 411: Document 411 (Score: 0.0000)\n",
            "Rank 412: Document 412 (Score: 0.0000)\n",
            "Rank 413: Document 413 (Score: 0.0000)\n",
            "Rank 414: Document 414 (Score: 0.0000)\n",
            "Rank 415: Document 415 (Score: 0.0000)\n",
            "Rank 416: Document 416 (Score: 0.0000)\n",
            "Rank 417: Document 417 (Score: 0.0000)\n",
            "Rank 418: Document 418 (Score: 0.0000)\n",
            "Rank 419: Document 419 (Score: 0.0000)\n",
            "Rank 420: Document 420 (Score: 0.0000)\n",
            "Rank 421: Document 421 (Score: 0.0000)\n",
            "Rank 422: Document 422 (Score: 0.0000)\n",
            "Rank 423: Document 423 (Score: 0.0000)\n",
            "Rank 424: Document 424 (Score: 0.0000)\n",
            "Rank 425: Document 425 (Score: 0.0000)\n",
            "Rank 426: Document 426 (Score: 0.0000)\n",
            "Rank 427: Document 427 (Score: 0.0000)\n",
            "Rank 428: Document 428 (Score: 0.0000)\n",
            "Rank 429: Document 429 (Score: 0.0000)\n",
            "Rank 430: Document 430 (Score: 0.0000)\n",
            "Rank 431: Document 431 (Score: 0.0000)\n",
            "Rank 432: Document 432 (Score: 0.0000)\n",
            "Rank 433: Document 433 (Score: 0.0000)\n",
            "Rank 434: Document 434 (Score: 0.0000)\n",
            "Rank 435: Document 435 (Score: 0.0000)\n",
            "Rank 436: Document 436 (Score: 0.0000)\n",
            "Rank 437: Document 437 (Score: 0.0000)\n",
            "Rank 438: Document 438 (Score: 0.0000)\n",
            "Rank 439: Document 439 (Score: 0.0000)\n",
            "Rank 440: Document 440 (Score: 0.0000)\n",
            "Rank 441: Document 441 (Score: 0.0000)\n",
            "Rank 442: Document 442 (Score: 0.0000)\n",
            "Rank 443: Document 443 (Score: 0.0000)\n",
            "Rank 444: Document 444 (Score: 0.0000)\n",
            "Rank 445: Document 445 (Score: 0.0000)\n",
            "Rank 446: Document 446 (Score: 0.0000)\n",
            "Rank 447: Document 447 (Score: 0.0000)\n",
            "Rank 448: Document 448 (Score: 0.0000)\n",
            "Rank 449: Document 449 (Score: 0.0000)\n",
            "Rank 450: Document 450 (Score: 0.0000)\n",
            "Rank 451: Document 451 (Score: 0.0000)\n",
            "Rank 452: Document 452 (Score: 0.0000)\n",
            "Rank 453: Document 453 (Score: 0.0000)\n",
            "Rank 454: Document 454 (Score: 0.0000)\n",
            "Rank 455: Document 455 (Score: 0.0000)\n",
            "Rank 456: Document 456 (Score: 0.0000)\n",
            "Rank 457: Document 457 (Score: 0.0000)\n",
            "Rank 458: Document 458 (Score: 0.0000)\n",
            "Rank 459: Document 459 (Score: 0.0000)\n",
            "Rank 460: Document 460 (Score: 0.0000)\n",
            "Rank 461: Document 461 (Score: 0.0000)\n",
            "Rank 462: Document 462 (Score: 0.0000)\n",
            "Rank 463: Document 463 (Score: 0.0000)\n",
            "Rank 464: Document 464 (Score: 0.0000)\n",
            "Rank 465: Document 465 (Score: 0.0000)\n",
            "Rank 466: Document 466 (Score: 0.0000)\n",
            "Rank 467: Document 467 (Score: 0.0000)\n",
            "Rank 468: Document 468 (Score: 0.0000)\n",
            "Rank 469: Document 469 (Score: 0.0000)\n",
            "Rank 470: Document 470 (Score: 0.0000)\n",
            "Rank 471: Document 471 (Score: 0.0000)\n",
            "Rank 472: Document 472 (Score: 0.0000)\n",
            "Rank 473: Document 473 (Score: 0.0000)\n",
            "Rank 474: Document 474 (Score: 0.0000)\n",
            "Rank 475: Document 475 (Score: 0.0000)\n",
            "Rank 476: Document 476 (Score: 0.0000)\n",
            "Rank 477: Document 477 (Score: 0.0000)\n",
            "Rank 478: Document 478 (Score: 0.0000)\n",
            "Rank 479: Document 479 (Score: 0.0000)\n",
            "Rank 480: Document 480 (Score: 0.0000)\n",
            "Rank 481: Document 481 (Score: 0.0000)\n",
            "Rank 482: Document 482 (Score: 0.0000)\n"
          ]
        }
      ],
      "source": [
        "# Execute Query Processing\n",
        "doc_vectors, query_vector, feature_names = compute_tfidf(documents, query)\n",
        "cosine_ranking = rank_by_cosine_similarity(doc_vectors, query_vector)\n",
        "\n",
        "# Display Results\n",
        "print(\"TF-IDF Cosine Similarity Ranking:\")\n",
        "for rank, (doc_index, score) in enumerate(cosine_ranking, 1):\n",
        "    print(f\"Rank {rank}: Document {doc_index} (Score: {score:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMKMWbL0W__K"
      },
      "source": [
        "- The output from the TF-IDF Cosine Similarity Ranking displays the ranking of documents based on their similarity to a specific query, utilizing the cosine similarity metric. Documents are scored and ranked in descending order based on their relevance to the query:\n",
        "\n",
        "   - Document 42 has the highest similarity score (0.4524), indicating that it is most relevant to the query.\n",
        "   - Document 53 and others follow with lower scores, reflecting their respective relevancies.\n",
        "   - Documents scoring 0.0000 appear to have no relevant terms in common with the query, implying no relevance or a very distant relationship to the query terms.\n",
        "   \n",
        "- This method helps in understanding which documents (or textual data points) are most relevant to a given query, allowing users to quickly identify the most pertinent information in a large dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cG1tGY-WKBN"
      },
      "source": [
        "- Binary Independence Model is a probabilistic model used in information retrieval to rank documents based on the likelihood of query terms appearing in each document. It operates under the assumption that the presence of one word is independent of the presence of any other word in the document (hence \"binary independence\").\n",
        "\n",
        "# Advantages of BIM\n",
        "1. Probabilistic Nature: BIM provides a framework for understanding the likelihood of relevance of a document to a given search query based on the probability of occurrence of query terms in the document.\n",
        "\n",
        "2. Relevance Scoring: It calculates a score based on the log-odds of term occurrences, which helps in distinguishing between documents that are more likely to be relevant to the user’s query.\n",
        "\n",
        "3. Simple yet Effective: Despite its simplifying assumptions, BIM can be quite effective in environments where term independence is a reasonable approximation.\n",
        "\n",
        "# Use Case\n",
        "- Information Retrieval: BIM is particularly useful in search engines and document retrieval systems where you need to rank documents by relevance to a query based on the content's terms.\n",
        "- Document Filtering: It can also be used to filter out documents that do not meet certain keyword criteria, enhancing search efficiency and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "h4RaEPdwQ06d"
      },
      "outputs": [],
      "source": [
        "# Function to rank documents based on the Binary Independence Model\n",
        "def rank_by_bim(documents, query):\n",
        "    \"\"\"\n",
        "    Ranks documents based on the Binary Independence Model (BIM), which considers\n",
        "    the probabilistic distribution of terms across documents.\n",
        "    - documents: Dictionary of {doc_id: content}.\n",
        "    - query: User query string.\n",
        "    Returns a sorted list of tuples (doc_id, score).\n",
        "    \"\"\"\n",
        "    # Split the query string into individual terms and convert to lowercase\n",
        "    query_terms = query.lower().split()\n",
        "    # Initialize an empty dictionary to store scores for each document\n",
        "    doc_scores = {}\n",
        "\n",
        "    # Iterate over each document in the dictionary\n",
        "    for doc_id, content in documents.items():\n",
        "        # Split document content into individual terms and convert to lowercase\n",
        "        terms = content.lower().split()\n",
        "        # Initialize the score for this document\n",
        "        score = 0\n",
        "\n",
        "        # Check each query term against the document's terms\n",
        "        for term in query_terms:\n",
        "            if term in terms:\n",
        "                # Count how many times the term appears in the document\n",
        "                count = terms.count(term)\n",
        "                # Calculate the probability of the term's occurrence in the document\n",
        "                p = count / len(terms)\n",
        "\n",
        "                # Logarithmic scoring to prevent underflow and avoid division by zero\n",
        "                if p < 1:  # Ensure we don't take log(0) which is undefined\n",
        "                    score += math.log((p + 1e-6) / (1 - p + 1e-6))  # Add a small epsilon to prevent log(0)\n",
        "                # Optional: output each term's contribution to the document's score for debugging\n",
        "                print(f\"Doc ID: {doc_id}, Term: {term}, Count: {count}, p: {p}, Score Contribution: {score}\")\n",
        "\n",
        "        # Store the calculated score for the document\n",
        "        doc_scores[doc_id] = score\n",
        "\n",
        "    # Sort documents by their scores in descending order and return\n",
        "    ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return ranked_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s8zxq_tSwDl",
        "outputId": "8d6fb372-5662-45a7-ee77-8cf1187cd18e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Doc ID: barkeepâ tom, Term: harry, Count: 1, p: 0.125, Score Contribution: -1.9459032919438028\n",
            "Doc ID: firenze, Term: harry, Count: 2, p: 0.04, Score Contribution: -3.1780298723265643\n",
            "Doc ID: fred, Term: harry, Count: 2, p: 0.0392156862745098, Score Contribution: -3.1986486586915857\n",
            "Doc ID: george, Term: harry, Count: 1, p: 0.01639344262295082, Score Contribution: -4.094284580748675\n",
            "Doc ID: goblin, Term: harry, Count: 1, p: 0.2, Score Contribution: -1.3862906111316093\n",
            "Doc ID: hagrid, Term: harry, Count: 8, p: 0.02666666666666667, Score Contribution: -3.5972757886882856\n",
            "Doc ID: harry, Term: hermione, Count: 7, p: 0.009615384615384616, Score Contribution: -4.634626003345489\n",
            "Doc ID: hermione, Term: harry, Count: 31, p: 0.056363636363636366, Score Contribution: -2.8178999966323337\n",
            "Doc ID: lee jordan, Term: harry, Count: 1, p: 0.07692307692307693, Score Contribution: -2.484894733205246\n",
            "Doc ID: malfoy, Term: harry, Count: 1, p: 0.009345794392523364, Score Contribution: -4.663333109269612\n",
            "Doc ID: mcgonagall, Term: harry, Count: 2, p: 0.020202020202020204, Score Contribution: -3.881515319786558\n",
            "Doc ID: mrs. weasley, Term: harry, Count: 1, p: 0.05, Score Contribution: -2.9444200319974625\n",
            "Doc ID: neville, Term: harry, Count: 1, p: 0.037037037037037035, Score Contribution: -3.258070576846975\n",
            "Doc ID: oiiver, Term: harry, Count: 1, p: 0.5, Score Contribution: 0.0\n",
            "Doc ID: quirrell, Term: harry, Count: 1, p: 0.008403361344537815, Score Contribution: -4.770566640019671\n",
            "Doc ID: ron, Term: harry, Count: 7, p: 0.01647058823529412, Score Contribution: -4.089511587772661\n",
            "Doc ID: ron, Term: hermione, Count: 6, p: 0.01411764705882353, Score Contribution: -8.335553221961266\n",
            "Doc ID: ron and harry, Term: hermione, Count: 1, p: 0.5, Score Contribution: 0.0\n",
            "Doc ID: seamus, Term: harry, Count: 1, p: 0.034482758620689655, Score Contribution: -3.3321765463094453\n",
            "Doc ID: sir nicholas, Term: hermione, Count: 1, p: 0.08333333333333333, Score Contribution: -2.397884363778866\n",
            "Doc ID: vernon, Term: harry, Count: 1, p: 0.08333333333333333, Score Contribution: -2.397884363778866\n",
            "Doc ID: voldemort, Term: harry, Count: 3, p: 0.0410958904109589, Score Contribution: -3.1498596632005658\n",
            "Doc ID: whispers, Term: harry, Count: 1, p: 0.5, Score Contribution: 0.0\n",
            "Doc ID: aunt petunia, Term: harry, Count: 3, p: 0.21428571428571427, Score Contribution: -1.299279590200946\n",
            "Doc ID: colin, Term: harry, Count: 2, p: 0.18181818181818182, Score Contribution: -1.5040731190128742\n",
            "Doc ID: diary, Term: harry, Count: 1, p: 0.0625, Score Contribution: -2.708035267896306\n",
            "Doc ID: dobby, Term: harry, Count: 18, p: 0.0594059405940594, Score Contribution: -2.762101652338161\n",
            "Doc ID: fudge, Term: harry, Count: 3, p: 0.016042780748663103, Score Contribution: -4.116262153854016\n",
            "Doc ID: gilderoy lockhart, Term: harry, Count: 8, p: 0.019753086419753086, Score Contribution: -3.90444513543937\n",
            "Doc ID: lockhart, Term: harry, Count: 3, p: 0.06818181818181818, Score Contribution: -2.614946184647242\n",
            "Doc ID: moaning myrtle, Term: harry, Count: 2, p: 0.01834862385321101, Score Contribution: -3.9796281740781017\n",
            "Doc ID: mr. weasley, Term: harry, Count: 5, p: 0.07352941176470588, Score Contribution: -2.5336842934144084\n",
            "Doc ID: mr. weasley, Term: hermione, Count: 1, p: 0.014705882352941176, Score Contribution: -6.738309930042128\n",
            "Doc ID: photographer, Term: harry, Count: 1, p: 0.06666666666666667, Score Contribution: -2.639043401155755\n",
            "Doc ID: tom riddle, Term: harry, Count: 3, p: 0.009345794392523364, Score Contribution: -4.663333109269612\n",
            "Doc ID: voice, Term: harry, Count: 1, p: 0.09090909090909091, Score Contribution: -2.30257519305394\n",
            "Doc ID: wood, Term: harry, Count: 1, p: 0.01639344262295082, Score Contribution: -4.094284580748675\n",
            "Doc ID: boy 1, Term: harry, Count: 1, p: 0.5, Score Contribution: 0.0\n",
            "Doc ID: lupin, Term: harry, Count: 17, p: 0.023578363384188627, Score Contribution: -3.7235236253836765\n",
            "Doc ID: lupin, Term: hermione, Count: 1, p: 0.0013869625520110957, Score Contribution: -10.302055098577798\n",
            "Doc ID: madam rosmerta, Term: harry, Count: 1, p: 0.025, Score Contribution: -3.663522672570125\n",
            "Doc ID: pettigrew, Term: harry, Count: 1, p: 0.017543859649122806, Score Contribution: -4.025295710216212\n",
            "Doc ID: sirius, Term: harry, Count: 3, p: 0.012345679012345678, Score Contribution: -4.381946650453692\n",
            "\n",
            "Binary Independence Model Ranking:\n",
            "Rank 1: all (Score: 0.0000)\n",
            "Rank 2: all 3 (Score: 0.0000)\n",
            "Rank 3: boy (Score: 0.0000)\n",
            "Rank 4: class (Score: 0.0000)\n",
            "Rank 5: crowd (Score: 0.0000)\n",
            "Rank 6: dean (Score: 0.0000)\n",
            "Rank 7: draco (Score: 0.0000)\n",
            "Rank 8: dudley (Score: 0.0000)\n",
            "Rank 9: dumbledore (Score: 0.0000)\n",
            "Rank 10: fat lady (Score: 0.0000)\n",
            "Rank 11: filch (Score: 0.0000)\n",
            "Rank 12: flint (Score: 0.0000)\n",
            "Rank 13: flitwick (Score: 0.0000)\n",
            "Rank 14: ginny (Score: 0.0000)\n",
            "Rank 15: girl (Score: 0.0000)\n",
            "Rank 16: griphook (Score: 0.0000)\n",
            "Rank 17: gryffindors (Score: 0.0000)\n",
            "Rank 18: hermoine (Score: 0.0000)\n",
            "Rank 19: madam hooch (Score: 0.0000)\n",
            "Rank 20: man (Score: 0.0000)\n",
            "Rank 21: man in paint (Score: 0.0000)\n",
            "Rank 22: oiiver (Score: 0.0000)\n",
            "Rank 23: oliver (Score: 0.0000)\n",
            "Rank 24: ollivander (Score: 0.0000)\n",
            "Rank 25: percy (Score: 0.0000)\n",
            "Rank 26: petunia (Score: 0.0000)\n",
            "Rank 27: ron and harry (Score: 0.0000)\n",
            "Rank 28: snake (Score: 0.0000)\n",
            "Rank 29: snape (Score: 0.0000)\n",
            "Rank 30: someone (Score: 0.0000)\n",
            "Rank 31: sorting hat (Score: 0.0000)\n",
            "Rank 32: students (Score: 0.0000)\n",
            "Rank 33: trainmaster (Score: 0.0000)\n",
            "Rank 34: whispers (Score: 0.0000)\n",
            "Rank 35: witch (Score: 0.0000)\n",
            "Rank 36: woman (Score: 0.0000)\n",
            "Rank 37: aragog (Score: 0.0000)\n",
            "Rank 38: auntâ petuniaâ & dudley (Score: 0.0000)\n",
            "Rank 39: cornish pixies (Score: 0.0000)\n",
            "Rank 40: crabbe (Score: 0.0000)\n",
            "Rank 41: fred, george, ron (Score: 0.0000)\n",
            "Rank 42: fred, george, ron, harry (Score: 0.0000)\n",
            "Rank 43: harry and ron (Score: 0.0000)\n",
            "Rank 44: harry-ron-hermione (Score: 0.0000)\n",
            "Rank 45: justin finch-fletchley (Score: 0.0000)\n",
            "Rank 46: lucius malfoy (Score: 0.0000)\n",
            "Rank 47: madam pomfrey (Score: 0.0000)\n",
            "Rank 48: mr. borgin (Score: 0.0000)\n",
            "Rank 49: penelope clearwater (Score: 0.0000)\n",
            "Rank 50: picture (Score: 0.0000)\n",
            "Rank 51: professor sprout (Score: 0.0000)\n",
            "Rank 52: slytherins (Score: 0.0000)\n",
            "Rank 53: student (Score: 0.0000)\n",
            "Rank 54: uncle vernon (Score: 0.0000)\n",
            "Rank 55: aunt marge (Score: 0.0000)\n",
            "Rank 56: bem (Score: 0.0000)\n",
            "Rank 57: boy 1 (Score: 0.0000)\n",
            "Rank 58: boy 2 (Score: 0.0000)\n",
            "Rank 59: fred & george (Score: 0.0000)\n",
            "Rank 60: goyle (Score: 0.0000)\n",
            "Rank 61: housekeeper (Score: 0.0000)\n",
            "Rank 62: pansy parkinson (Score: 0.0000)\n",
            "Rank 63: parvati (Score: 0.0000)\n",
            "Rank 64: shrunken head (Score: 0.0000)\n",
            "Rank 65: shrunken head 1 (Score: 0.0000)\n",
            "Rank 66: shrunken head 2 (Score: 0.0000)\n",
            "Rank 67: stan shunpike (Score: 0.0000)\n",
            "Rank 68: teacher (Score: 0.0000)\n",
            "Rank 69: tom (Score: 0.0000)\n",
            "Rank 70: trelawney (Score: 0.0000)\n",
            "Rank 71: vendor (Score: 0.0000)\n",
            "Rank 72: after addition of final ingredient: taste and colour vary depending on the person being turned into\";advanced,,,,,,,, (Score: 0.0000)\n",
            "Rank 73: ageing potion (Score: 0.0000)\n",
            "Rank 74: amortentia (Score: 0.0000)\n",
            "Rank 75: antidote to veritaserum (Score: 0.0000)\n",
            "Rank 76: babbling beverage (Score: 0.0000)\n",
            "Rank 77: baruffio's brain elixir (Score: 0.0000)\n",
            "Rank 78: befuddlement draught (Score: 0.0000)\n",
            "Rank 79: blood-replenishing potion (Score: 0.0000)\n",
            "Rank 80: bruise removal paste (Score: 0.0000)\n",
            "Rank 81: bundimun secretion (Score: 0.0000)\n",
            "Rank 82: burn-healing paste (Score: 0.0000)\n",
            "Rank 83: calming draught (Score: 0.0000)\n",
            "Rank 84: caxambu style borborygmus potion (Score: 0.0000)\n",
            "Rank 85: chelidonium miniscula (Score: 0.0000)\n",
            "Rank 86: confusing concoction (Score: 0.0000)\n",
            "Rank 87: cough potion (Score: 0.0000)\n",
            "Rank 88: cupid crystals (Score: 0.0000)\n",
            "Rank 89: cure for boils (Score: 0.0000)\n",
            "Rank 90: death potion (Score: 0.0000)\n",
            "Rank 91: deflating draught (Score: 0.0000)\n",
            "Rank 92: developing solution (Score: 0.0000)\n",
            "Rank 93: doxycide (Score: 0.0000)\n",
            "Rank 94: dr ubbly's oblivious unction (Score: 0.0000)\n",
            "Rank 95: draught of living death (Score: 0.0000)\n",
            "Rank 96: draught of peace (Score: 0.0000)\n",
            "Rank 97: elixir of life (Score: 0.0000)\n",
            "Rank 98: elixir to induce euphoria (Score: 0.0000)\n",
            "Rank 99: emerald potion (Score: 0.0000)\n",
            "Rank 100: essence of insanity (Score: 0.0000)\n",
            "Rank 101: everlasting elixirs (Score: 0.0000)\n",
            "Rank 102: fake protective potions (Score: 0.0000)\n",
            "Rank 103: felix felicis (Score: 0.0000)\n",
            "Rank 104: fergus fungal budge (Score: 0.0000)\n",
            "Rank 105: fire protection potion (Score: 0.0000)\n",
            "Rank 106: first love beguiling bubbles (Score: 0.0000)\n",
            "Rank 107: flesh-eating slug repellent (Score: 0.0000)\n",
            "Rank 108: forgetfulness potion (Score: 0.0000)\n",
            "Rank 109: garrotting gas (Score: 0.0000)\n",
            "Rank 110: hair-raising potion (Score: 0.0000)\n",
            "Rank 111: heartbreak teardrops (Score: 0.0000)\n",
            "Rank 112: herbicide potion (Score: 0.0000)\n",
            "Rank 113: hiccoughing solution (Score: 0.0000)\n",
            "Rank 114: honeywater, vervain, infusion, scurvy grass,\" lovage\"\" (Score: 0.0000)\n",
            "Rank 115: invigoration draught;\"alihotsy leaves, dried billywig stings, peppermint, stewed mandrake, infusion of wormwood,,,, (Score: 0.0000)\n",
            "Rank 116: kissing concoction (Score: 0.0000)\n",
            "Rank 117: laxative potion (Score: 0.0000)\n",
            "Rank 118: love potion antidote (Score: 0.0000)\n",
            "Rank 119: madame glossy's silver polish (Score: 0.0000)\n",
            "Rank 120: mandrake restorative draught (Score: 0.0000)\n",
            "Rank 121: mouth itching antidote (Score: 0.0000)\n",
            "Rank 122: mrs skower's all-purpose magical mess remover (Score: 0.0000)\n",
            "Rank 123: murtlap essence (Score: 0.0000)\n",
            "Rank 124: pepperup potion (Score: 0.0000)\n",
            "Rank 125: polyjuice potion (Score: 0.0000)\n",
            "Rank 126: potion for dreamless sleep (Score: 0.0000)\n",
            "Rank 127: rat tonic (Score: 0.0000)\n",
            "Rank 128: regeneration potion (Score: 0.0000)\n",
            "Rank 129: rudimentary body potion (Score: 0.0000)\n",
            "Rank 130: shrinking solution (Score: 0.0000)\n",
            "Rank 131: skele-gro (Score: 0.0000)\n",
            "Rank 132: sleekeazy's hair potion (Score: 0.0000)\n",
            "Rank 133: sleeping draught (Score: 0.0000)\n",
            "Rank 134: strengthening solution (Score: 0.0000)\n",
            "Rank 135: swelling solution (Score: 0.0000)\n",
            "Rank 136: ten-second pimple vanisher (Score: 0.0000)\n",
            "Rank 137: thick golden potion (Score: 0.0000)\n",
            "Rank 138: tolipan blemish blitzer (Score: 0.0000)\n",
            "Rank 139: truth serum (Score: 0.0000)\n",
            "Rank 140: veritaserum (Score: 0.0000)\n",
            "Rank 141: weedosoros (Score: 0.0000)\n",
            "Rank 142: wideye potion (Score: 0.0000)\n",
            "Rank 143: wit-sharpening potion (Score: 0.0000)\n",
            "Rank 144: wolfsbane potion;wolfsbane;\"eases the symptoms of lycanthropy; preventsâ werewolvesâ from losing their minds post-transformation.\";faint blue smoke when completed, unpleasant taste (Score: 0.0000)\n",
            "Rank 145: wound-cleaning potion (Score: 0.0000)\n",
            "Rank 146: age line (Score: 0.0000)\n",
            "Rank 147: albus dumbledore's forceful spell (Score: 0.0000)\n",
            "Rank 148: amplifying charm (Score: 0.0000)\n",
            "Rank 149: anti-cheating spell (Score: 0.0000)\n",
            "Rank 150: anti-disapparition jinx (Score: 0.0000)\n",
            "Rank 151: anti-intruder jinx (Score: 0.0000)\n",
            "Rank 152: antonin dolohov's curse (Score: 0.0000)\n",
            "Rank 153: apparition (Score: 0.0000)\n",
            "Rank 154: aqua eructo charm (Score: 0.0000)\n",
            "Rank 155: arrow-shooting spell (Score: 0.0000)\n",
            "Rank 156: ascendio (Score: 0.0000)\n",
            "Rank 157: avenseguim (Score: 0.0000)\n",
            "Rank 158: avifors spell (Score: 0.0000)\n",
            "Rank 159: babbling curse (Score: 0.0000)\n",
            "Rank 160: badgering (Score: 0.0000)\n",
            "Rank 161: bandaging charm (Score: 0.0000)\n",
            "Rank 162: banishing charm (Score: 0.0000)\n",
            "Rank 163: bat-bogey hex (Score: 0.0000)\n",
            "Rank 164: baubillious (Score: 0.0000)\n",
            "Rank 165: bedazzling hex (Score: 0.0000)\n",
            "Rank 166: bewitched snowballs (Score: 0.0000)\n",
            "Rank 167: bird-conjuring charm (Score: 0.0000)\n",
            "Rank 168: blasting curse (Score: 0.0000)\n",
            "Rank 169: blue sparks (Score: 0.0000)\n",
            "Rank 170: bluebell flames (Score: 0.0000)\n",
            "Rank 171: boggart-banishing spell (Score: 0.0000)\n",
            "Rank 172: brackium emendo (Score: 0.0000)\n",
            "Rank 173: bravery charm (Score: 0.0000)\n",
            "Rank 174: bridge-conjuring spell (Score: 0.0000)\n",
            "Rank 175: broom jinx (Score: 0.0000)\n",
            "Rank 176: bubble-head charm (Score: 0.0000)\n",
            "Rank 177: bubble-producing spell (Score: 0.0000)\n",
            "Rank 178: cantis (Score: 0.0000)\n",
            "Rank 179: cascading jinx (Score: 0.0000)\n",
            "Rank 180: caterwauling charm (Score: 0.0000)\n",
            "Rank 181: cauldron to badger (Score: 0.0000)\n",
            "Rank 182: cauldron to sieve (Score: 0.0000)\n",
            "Rank 183: cave inimicum (Score: 0.0000)\n",
            "Rank 184: cheering charm (Score: 0.0000)\n",
            "Rank 185: circumrota (Score: 0.0000)\n",
            "Rank 186: cistem aperio (Score: 0.0000)\n",
            "Rank 187: colour change charm (Score: 0.0000)\n",
            "Rank 188: confundus charm (Score: 0.0000)\n",
            "Rank 189: conjunctivitis curse (Score: 0.0000)\n",
            "Rank 190: cornflake skin spell (Score: 0.0000)\n",
            "Rank 191: cracker jinx (Score: 0.0000)\n",
            "Rank 192: cribbing spell (Score: 0.0000)\n",
            "Rank 193: crinus muto (Score: 0.0000)\n",
            "Rank 194: cruciatus curse (Score: 0.0000)\n",
            "Rank 195: curse of the bogies (Score: 0.0000)\n",
            "Rank 196: cushioning charm (Score: 0.0000)\n",
            "Rank 197: dancing feet spell (Score: 0.0000)\n",
            "Rank 198: dark mark (Score: 0.0000)\n",
            "Rank 199: densaugeo (Score: 0.0000)\n",
            "Rank 200: deprimo (Score: 0.0000)\n",
            "Rank 201: descendo (Score: 0.0000)\n",
            "Rank 202: desk into pig (Score: 0.0000)\n",
            "Rank 203: deterioration hex (Score: 0.0000)\n",
            "Rank 204: diminuendo (Score: 0.0000)\n",
            "Rank 205: disarming charm (Score: 0.0000)\n",
            "Rank 206: disillusionment charm (Score: 0.0000)\n",
            "Rank 207: dissendium (Score: 0.0000)\n",
            "Rank 208: doubling charm (Score: 0.0000)\n",
            "Rank 209: draconifors spell (Score: 0.0000)\n",
            "Rank 210: drought charm (Score: 0.0000)\n",
            "Rank 211: ducklifors jinx (Score: 0.0000)\n",
            "Rank 212: dumbledore's army parchment jinx (Score: 0.0000)\n",
            "Rank 213: ear-shrivelling curse (Score: 0.0000)\n",
            "Rank 214: ears to kumquats (Score: 0.0000)\n",
            "Rank 215: ebublio jinx (Score: 0.0000)\n",
            "Rank 216: engorgement charm (Score: 0.0000)\n",
            "Rank 217: engorgio skullus (Score: 0.0000)\n",
            "Rank 218: entrail-expelling curse (Score: 0.0000)\n",
            "Rank 219: episkey (Score: 0.0000)\n",
            "Rank 220: epoximise (Score: 0.0000)\n",
            "Rank 221: eradication spell (Score: 0.0000)\n",
            "Rank 222: erecto (Score: 0.0000)\n",
            "Rank 223: evanesce (Score: 0.0000)\n",
            "Rank 224: everte statum (Score: 0.0000)\n",
            "Rank 225: exploding charm (Score: 0.0000)\n",
            "Rank 226: exploding charm 2 (Score: 0.0000)\n",
            "Rank 227: expulso curse (Score: 0.0000)\n",
            "Rank 228: extension charm (Score: 0.0000)\n",
            "Rank 229: extinguishing spell (Score: 0.0000)\n",
            "Rank 230: false memory charm (Score: 0.0000)\n",
            "Rank 231: feather-light charm (Score: 0.0000)\n",
            "Rank 232: ferret to human (Score: 0.0000)\n",
            "Rank 233: fianto duri (Score: 0.0000)\n",
            "Rank 234: fidelius charm (Score: 0.0000)\n",
            "Rank 235: fiendfyre (Score: 0.0000)\n",
            "Rank 236: finestra spell (Score: 0.0000)\n",
            "Rank 237: finger-removing jinx (Score: 0.0000)\n",
            "Rank 238: fire-making spell (Score: 0.0000)\n",
            "Rank 239: firestorm (Score: 0.0000)\n",
            "Rank 240: flagrante curse (Score: 0.0000)\n",
            "Rank 241: flagrate (Score: 0.0000)\n",
            "Rank 242: flame-freezing charm (Score: 0.0000)\n",
            "Rank 243: flask-conjuring spell (Score: 0.0000)\n",
            "Rank 244: flintifors (Score: 0.0000)\n",
            "Rank 245: flipendo tria (Score: 0.0000)\n",
            "Rank 246: flying charm (Score: 0.0000)\n",
            "Rank 247: four-point spell (Score: 0.0000)\n",
            "Rank 248: freezing charm (Score: 0.0000)\n",
            "Rank 249: freezing spell (Score: 0.0000)\n",
            "Rank 250: full body-bind curse (Score: 0.0000)\n",
            "Rank 251: fumos duo (Score: 0.0000)\n",
            "Rank 252: fur spell (Score: 0.0000)\n",
            "Rank 253: general counter-spell (Score: 0.0000)\n",
            "Rank 254: glacius duo (Score: 0.0000)\n",
            "Rank 255: glacius tria (Score: 0.0000)\n",
            "Rank 256: glisseo (Score: 0.0000)\n",
            "Rank 257: gouging spell (Score: 0.0000)\n",
            "Rank 258: green sparks (Score: 0.0000)\n",
            "Rank 259: gripping charm (Score: 0.0000)\n",
            "Rank 260: hair loss curse (Score: 0.0000)\n",
            "Rank 261: hair-thickening charm (Score: 0.0000)\n",
            "Rank 262: hardening charm (Score: 0.0000)\n",
            "Rank 263: harmonia nectere passus (Score: 0.0000)\n",
            "Rank 264: head shrink spell (Score: 0.0000)\n",
            "Rank 265: healing spell (Score: 0.0000)\n",
            "Rank 266: herbifors spell (Score: 0.0000)\n",
            "Rank 267: herbivicus charm (Score: 0.0000)\n",
            "Rank 268: hex that grows antlers on the head (Score: 0.0000)\n",
            "Rank 269: homing spell (Score: 0.0000)\n",
            "Rank 270: homonculous charm (Score: 0.0000)\n",
            "Rank 271: homorphus charm (Score: 0.0000)\n",
            "Rank 272: horcrux-making spell (Score: 0.0000)\n",
            "Rank 273: horn tongue hex (Score: 0.0000)\n",
            "Rank 274: horton-keitch braking charm (Score: 0.0000)\n",
            "Rank 275: hot-air charm (Score: 0.0000)\n",
            "Rank 276: hour-reversal charm (Score: 0.0000)\n",
            "Rank 277: hover charm (Score: 0.0000)\n",
            "Rank 278: human presence revealing spell (Score: 0.0000)\n",
            "Rank 279: hurling hex (Score: 0.0000)\n",
            "Rank 280: illegibilus (Score: 0.0000)\n",
            "Rank 281: impediment jinx (Score: 0.0000)\n",
            "Rank 282: imperius curse (Score: 0.0000)\n",
            "Rank 283: imperturbable charm (Score: 0.0000)\n",
            "Rank 284: impervius charm (Score: 0.0000)\n",
            "Rank 285: inanimatus conjurus spell (Score: 0.0000)\n",
            "Rank 286: incarcerous spell (Score: 0.0000)\n",
            "Rank 287: incendio duo spell (Score: 0.0000)\n",
            "Rank 288: incendio tria (Score: 0.0000)\n",
            "Rank 289: inflating charm (Score: 0.0000)\n",
            "Rank 290: informous spell (Score: 0.0000)\n",
            "Rank 291: insect jinx (Score: 0.0000)\n",
            "Rank 292: instant scalping hex (Score: 0.0000)\n",
            "Rank 293: intruder charm (Score: 0.0000)\n",
            "Rank 294: jelly-brain jinx (Score: 0.0000)\n",
            "Rank 295: jelly-fingers curse (Score: 0.0000)\n",
            "Rank 296: jelly-legs curse (Score: 0.0000)\n",
            "Rank 297: killing curse (Score: 0.0000)\n",
            "Rank 298: knee-reversal hex (Score: 0.0000)\n",
            "Rank 299: knockback jinx (Score: 0.0000)\n",
            "Rank 300: knockback jinx duo (Score: 0.0000)\n",
            "Rank 301: lacarnum inflamari (Score: 0.0000)\n",
            "Rank 302: langlock (Score: 0.0000)\n",
            "Rank 303: lapifors spell (Score: 0.0000)\n",
            "Rank 304: launch an object up into the air (Score: 0.0000)\n",
            "Rank 305: leek jinx (Score: 0.0000)\n",
            "Rank 306: leg-locker curse (Score: 0.0000)\n",
            "Rank 307: legilimency spell (Score: 0.0000)\n",
            "Rank 308: levicorpus (Score: 0.0000)\n",
            "Rank 309: levitation charm (Score: 0.0000)\n",
            "Rank 310: liberacorpus (Score: 0.0000)\n",
            "Rank 311: locking spell (Score: 0.0000)\n",
            "Rank 312: locomotion charm (Score: 0.0000)\n",
            "Rank 313: lumos maxima (Score: 0.0000)\n",
            "Rank 314: lumos solem spell (Score: 0.0000)\n",
            "Rank 315: magicus extremos (Score: 0.0000)\n",
            "Rank 316: melofors jinx (Score: 0.0000)\n",
            "Rank 317: memory charm (Score: 0.0000)\n",
            "Rank 318: mending charm (Score: 0.0000)\n",
            "Rank 319: meteolojinx recanto (Score: 0.0000)\n",
            "Rank 320: minerva mcgonagall's fire-creating spell (Score: 0.0000)\n",
            "Rank 321: ministry of magic fog (Score: 0.0000)\n",
            "Rank 322: mobiliarbus (Score: 0.0000)\n",
            "Rank 323: mobilicorpus (Score: 0.0000)\n",
            "Rank 324: muffliato charm (Score: 0.0000)\n",
            "Rank 325: muggle-repelling charm (Score: 0.0000)\n",
            "Rank 326: multicorfors spell (Score: 0.0000)\n",
            "Rank 327: mutatio skullus (Score: 0.0000)\n",
            "Rank 328: nebulus (Score: 0.0000)\n",
            "Rank 329: obliteration charm (Score: 0.0000)\n",
            "Rank 330: obscuro (Score: 0.0000)\n",
            "Rank 331: oculus reparo (Score: 0.0000)\n",
            "Rank 332: oppugno jinx (Score: 0.0000)\n",
            "Rank 333: orbis jinx (Score: 0.0000)\n",
            "Rank 334: orchideous (Score: 0.0000)\n",
            "Rank 335: oscausi (Score: 0.0000)\n",
            "Rank 336: pack (Score: 0.0000)\n",
            "Rank 337: papyrus reparo (Score: 0.0000)\n",
            "Rank 338: partis temporus (Score: 0.0000)\n",
            "Rank 339: patented daydream charm (Score: 0.0000)\n",
            "Rank 340: patronus charm (Score: 0.0000)\n",
            "Rank 341: pepper breath hex (Score: 0.0000)\n",
            "Rank 342: periculum (Score: 0.0000)\n",
            "Rank 343: permanent sticking charm (Score: 0.0000)\n",
            "Rank 344: peskipiksi pesternomi (Score: 0.0000)\n",
            "Rank 345: piertotum locomotor (Score: 0.0000)\n",
            "Rank 346: pimple jinx (Score: 0.0000)\n",
            "Rank 347: piscifors (Score: 0.0000)\n",
            "Rank 348: placement charm (Score: 0.0000)\n",
            "Rank 349: portus (Score: 0.0000)\n",
            "Rank 350: protean charm (Score: 0.0000)\n",
            "Rank 351: protego diabolica (Score: 0.0000)\n",
            "Rank 352: protego horribilis (Score: 0.0000)\n",
            "Rank 353: protego maxima (Score: 0.0000)\n",
            "Rank 354: protego totalum (Score: 0.0000)\n",
            "Rank 355: purple firecrackers (Score: 0.0000)\n",
            "Rank 356: pus-squirting hex (Score: 0.0000)\n",
            "Rank 357: quietening charm (Score: 0.0000)\n",
            "Rank 358: red sparks (Score: 0.0000)\n",
            "Rank 359: reductor curse (Score: 0.0000)\n",
            "Rank 360: refilling charm (Score: 0.0000)\n",
            "Rank 361: reparifarge (Score: 0.0000)\n",
            "Rank 362: reparifors (Score: 0.0000)\n",
            "Rank 363: repello inimicum (Score: 0.0000)\n",
            "Rank 364: revealing charm (Score: 0.0000)\n",
            "Rank 365: revelio charm (Score: 0.0000)\n",
            "Rank 366: reverse spell (Score: 0.0000)\n",
            "Rank 367: reverte (Score: 0.0000)\n",
            "Rank 368: reviving spell (Score: 0.0000)\n",
            "Rank 369: revulsion jinx (Score: 0.0000)\n",
            "Rank 370: rose growth (Score: 0.0000)\n",
            "Rank 371: rowboat spell (Score: 0.0000)\n",
            "Rank 372: salvio hexia (Score: 0.0000)\n",
            "Rank 373: sardine hex (Score: 0.0000)\n",
            "Rank 374: sauce-making spell (Score: 0.0000)\n",
            "Rank 375: scouring charm (Score: 0.0000)\n",
            "Rank 376: sea urchin jinx (Score: 0.0000)\n",
            "Rank 377: sealant charm (Score: 0.0000)\n",
            "Rank 378: sectumsempra (Score: 0.0000)\n",
            "Rank 379: seize and pull charm (Score: 0.0000)\n",
            "Rank 380: severing charm (Score: 0.0000)\n",
            "Rank 381: shield charm (Score: 0.0000)\n",
            "Rank 382: shield penetration spell (Score: 0.0000)\n",
            "Rank 383: shooting spell (Score: 0.0000)\n",
            "Rank 384: shrinking charm (Score: 0.0000)\n",
            "Rank 385: silencing charm (Score: 0.0000)\n",
            "Rank 386: skurge charm (Score: 0.0000)\n",
            "Rank 387: slippery jinx (Score: 0.0000)\n",
            "Rank 388: slowing charm (Score: 0.0000)\n",
            "Rank 389: slug-vomiting charm (Score: 0.0000)\n",
            "Rank 390: smashing spell (Score: 0.0000)\n",
            "Rank 391: smokescreen spell (Score: 0.0000)\n",
            "Rank 392: snake summons spell (Score: 0.0000)\n",
            "Rank 393: snake-vanishing spell (Score: 0.0000)\n",
            "Rank 394: softening charm (Score: 0.0000)\n",
            "Rank 395: sonorous charm (Score: 0.0000)\n",
            "Rank 396: specialis revelio (Score: 0.0000)\n",
            "Rank 397: spider repelling spell (Score: 0.0000)\n",
            "Rank 398: sponge-knees curse (Score: 0.0000)\n",
            "Rank 399: squiggle quill (Score: 0.0000)\n",
            "Rank 400: stealth sensoring spell (Score: 0.0000)\n",
            "Rank 401: steleus (Score: 0.0000)\n",
            "Rank 402: stickfast hex (Score: 0.0000)\n",
            "Rank 403: stinging jinx (Score: 0.0000)\n",
            "Rank 404: stretching jinx (Score: 0.0000)\n",
            "Rank 405: stunning spell (Score: 0.0000)\n",
            "Rank 406: summoning charm (Score: 0.0000)\n",
            "Rank 407: supersensory charm (Score: 0.0000)\n",
            "Rank 408: surgito (Score: 0.0000)\n",
            "Rank 409: switching spell (Score: 0.0000)\n",
            "Rank 410: taboo (Score: 0.0000)\n",
            "Rank 411: tail-growing hex (Score: 0.0000)\n",
            "Rank 412: teacup to tortoise (Score: 0.0000)\n",
            "Rank 413: teapot to tortoise (Score: 0.0000)\n",
            "Rank 414: teeth-straightening spell (Score: 0.0000)\n",
            "Rank 415: teleportation spell (Score: 0.0000)\n",
            "Rank 416: tentaclifors (Score: 0.0000)\n",
            "Rank 417: tergeo (Score: 0.0000)\n",
            "Rank 418: tickling charm (Score: 0.0000)\n",
            "Rank 419: tickling hex (Score: 0.0000)\n",
            "Rank 420: toenail-growing hex (Score: 0.0000)\n",
            "Rank 421: tongue-tying curse (Score: 0.0000)\n",
            "Rank 422: tracking spell (Score: 0.0000)\n",
            "Rank 423: transmogrifian torture (Score: 0.0000)\n",
            "Rank 424: trip jinx (Score: 0.0000)\n",
            "Rank 425: twitchy-ears hex (Score: 0.0000)\n",
            "Rank 426: unbreakable charm (Score: 0.0000)\n",
            "Rank 427: unbreakable vow (Score: 0.0000)\n",
            "Rank 428: unlocking charm (Score: 0.0000)\n",
            "Rank 429: unsupported flight (Score: 0.0000)\n",
            "Rank 430: vacuum cleaner spell (Score: 0.0000)\n",
            "Rank 431: vanishing spell (Score: 0.0000)\n",
            "Rank 432: ventus duo (Score: 0.0000)\n",
            "Rank 433: ventus jinx (Score: 0.0000)\n",
            "Rank 434: vera verto (Score: 0.0000)\n",
            "Rank 435: verdillious (Score: 0.0000)\n",
            "Rank 436: verdimillious charm (Score: 0.0000)\n",
            "Rank 437: verdimillious duo spell (Score: 0.0000)\n",
            "Rank 438: vermiculus jinx (Score: 0.0000)\n",
            "Rank 439: vulnera sanentur (Score: 0.0000)\n",
            "Rank 440: waddiwasi (Score: 0.0000)\n",
            "Rank 441: wand-extinguishing charm (Score: 0.0000)\n",
            "Rank 442: wand-lighting charm (Score: 0.0000)\n",
            "Rank 443: wand-lighting charm duo (Score: 0.0000)\n",
            "Rank 444: washing up spell (Score: 0.0000)\n",
            "Rank 445: water-making spell (Score: 0.0000)\n",
            "Rank 446: white sparks (Score: 0.0000)\n",
            "Rank 447: aunt petunia (Score: -1.2993)\n",
            "Rank 448: goblin (Score: -1.3863)\n",
            "Rank 449: colin (Score: -1.5041)\n",
            "Rank 450: barkeepâ tom (Score: -1.9459)\n",
            "Rank 451: voice (Score: -2.3026)\n",
            "Rank 452: sir nicholas (Score: -2.3979)\n",
            "Rank 453: vernon (Score: -2.3979)\n",
            "Rank 454: lee jordan (Score: -2.4849)\n",
            "Rank 455: lockhart (Score: -2.6149)\n",
            "Rank 456: photographer (Score: -2.6390)\n",
            "Rank 457: diary (Score: -2.7080)\n",
            "Rank 458: dobby (Score: -2.7621)\n",
            "Rank 459: hermione (Score: -2.8179)\n",
            "Rank 460: mrs. weasley (Score: -2.9444)\n",
            "Rank 461: voldemort (Score: -3.1499)\n",
            "Rank 462: firenze (Score: -3.1780)\n",
            "Rank 463: fred (Score: -3.1986)\n",
            "Rank 464: neville (Score: -3.2581)\n",
            "Rank 465: seamus (Score: -3.3322)\n",
            "Rank 466: hagrid (Score: -3.5973)\n",
            "Rank 467: madam rosmerta (Score: -3.6635)\n",
            "Rank 468: mcgonagall (Score: -3.8815)\n",
            "Rank 469: gilderoy lockhart (Score: -3.9044)\n",
            "Rank 470: moaning myrtle (Score: -3.9796)\n",
            "Rank 471: pettigrew (Score: -4.0253)\n",
            "Rank 472: george (Score: -4.0943)\n",
            "Rank 473: wood (Score: -4.0943)\n",
            "Rank 474: fudge (Score: -4.1163)\n",
            "Rank 475: sirius (Score: -4.3819)\n",
            "Rank 476: harry (Score: -4.6346)\n",
            "Rank 477: malfoy (Score: -4.6633)\n",
            "Rank 478: tom riddle (Score: -4.6633)\n",
            "Rank 479: quirrell (Score: -4.7706)\n",
            "Rank 480: mr. weasley (Score: -6.7383)\n",
            "Rank 481: ron (Score: -8.3356)\n",
            "Rank 482: lupin (Score: -10.3021)\n"
          ]
        }
      ],
      "source": [
        "# Executing the query\n",
        "bim_ranking = rank_by_bim(documents, query)\n",
        "\n",
        "# Display Results\n",
        "print(\"\\nBinary Independence Model Ranking:\")\n",
        "for rank, (doc_id, score) in enumerate(bim_ranking, 1):\n",
        "    print(f\"Rank {rank}: {doc_id} (Score: {score:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROt71JD0Wo8x"
      },
      "source": [
        "- The output from the Binary Independence Model (BIM) scoring system lists each document (or in this context, possibly characters and terms from a dataset) along with a score that indicates their relevance to the query terms \"Harry\" and \"Hermione.\" The scores are calculated based on the frequency of query terms in the documents, adjusted by the overall length of each document. Documents (or entries) with higher scores are deemed more relevant to the query, though many entries have scores of zero, indicating no relevance or a lack of the query terms. Higher negative scores represent lesser relevance or rare occurrences of the query terms relative to the size of the document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX4YE2BSXY5J"
      },
      "source": [
        "# Step 6: Query Expansion Using Bigrams\n",
        "\n",
        "- This step enhances user queries by suggesting related terms based on bigrams (two-word combinations). It expands the original query with these related terms and reranks the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "f4F0Rc-wSs1o"
      },
      "outputs": [],
      "source": [
        "# Function to generate bigrams from tokens\n",
        "def generate_bigrams(tokens):\n",
        "    \"\"\"\n",
        "    Generates bigrams from a list of tokens.\n",
        "    - tokens: List of tokenized words.\n",
        "    Returns a list of bigram tuples.\n",
        "    \"\"\"\n",
        "    return [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "E1458vI4XxXA"
      },
      "outputs": [],
      "source": [
        "# Build a Bigram Index from the Corpus\n",
        "def build_bigram_index(corpus):\n",
        "    \"\"\"\n",
        "    Constructs a bigram index from the input corpus.\n",
        "    - corpus: Dictionary where keys are document IDs and values are document content.\n",
        "    Returns a dictionary where:\n",
        "        - Keys are bigram tuples.\n",
        "        - Values are lists of document IDs containing the bigram.\n",
        "    \"\"\"\n",
        "    bigram_index = defaultdict(set)\n",
        "    for doc_id, content in corpus.items():\n",
        "        tokens = tokenize_and_normalize(content)\n",
        "        bigrams = generate_bigrams(tokens)\n",
        "        for bigram in bigrams:\n",
        "            bigram_index[bigram].add(doc_id)\n",
        "    return {bigram: list(doc_ids) for bigram, doc_ids in bigram_index.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ayhEYONmX6m3"
      },
      "outputs": [],
      "source": [
        "# Suggest Related Terms Based on Bigrams\n",
        "def suggest_related_terms(query, bigram_index, max_suggestions=5):\n",
        "    \"\"\"\n",
        "    Suggests related terms for the query based on bigrams.\n",
        "    - query: User query string.\n",
        "    - bigram_index: Precomputed bigram index.\n",
        "    - max_suggestions: Maximum number of suggestions to return.\n",
        "    Returns a list of suggested terms.\n",
        "    \"\"\"\n",
        "    query_tokens = tokenize_and_normalize(query)\n",
        "    query_bigrams = generate_bigrams(query_tokens)\n",
        "    term_counts = defaultdict(int)\n",
        "\n",
        "    # Count the occurrences of terms related to query bigrams\n",
        "    for bigram in query_bigrams:\n",
        "        if bigram in bigram_index:\n",
        "            for related_doc_id in bigram_index[bigram]:\n",
        "                for related_bigram in bigram_index:\n",
        "                    if related_doc_id in bigram_index[related_bigram]:\n",
        "                        term_counts[related_bigram[1]] += 1\n",
        "\n",
        "    # Sort terms by frequency and limit the number of suggestions\n",
        "    sorted_terms = sorted(term_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [term for term, _ in sorted_terms[:max_suggestions]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Ys7gYp5_YIPh"
      },
      "outputs": [],
      "source": [
        "# Expand the Query with Suggestions\n",
        "def expand_query(query, suggestions):\n",
        "    \"\"\"\n",
        "    Expands the query by appending suggested terms.\n",
        "    - query: Original query string.\n",
        "    - suggestions: List of suggested terms.\n",
        "    Returns the expanded query string.\n",
        "    \"\"\"\n",
        "    return query + \" \" + \" \".join(suggestions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63MGXLd-YNAL",
        "outputId": "41c7a6ca-dad8-444b-ab54-a98365358b22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Query: Harry Potter magic\n",
            "Suggested Terms: None\n",
            "Expanded Query: Harry Potter magic \n"
          ]
        }
      ],
      "source": [
        "# Example Usage\n",
        "try:\n",
        "    bigram_index = build_bigram_index(structured_corpus)\n",
        "    query = \"Harry Potter magic\"\n",
        "    suggestions = suggest_related_terms(query, bigram_index)\n",
        "    expanded_query = expand_query(query, suggestions)\n",
        "\n",
        "    print(f\"Original Query: {query}\")\n",
        "    print(f\"Suggested Terms: {', '.join(suggestions) if suggestions else 'None'}\")\n",
        "    print(f\"Expanded Query: {expanded_query}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during query expansion: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stGm1Mq0YTkk",
        "outputId": "e99a5a9d-0de3-4d0e-9d16-60495c8e40eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents related to 'Harry Potter Magic': {'diary', 'firenze', 'madam rosmerta', 'lockhart', 'malfoy', 'barkeepâ tom', 'goblin', 'fudge', 'lee jordan', 'voice', 'vernon', 'dobby', 'tom riddle', 'voldemort', 'photographer', 'mr. weasley', 'whispers', 'quirrell'}\n"
          ]
        }
      ],
      "source": [
        "def load_documents(file_paths):\n",
        "    documents = {}\n",
        "    for path in file_paths:\n",
        "        with open(path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            documents.update(data)\n",
        "    return documents\n",
        "\n",
        "def tokenize_and_normalize(text):\n",
        "    if isinstance(text, list):\n",
        "        text = ' '.join(text)\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "def generate_bigrams(tokens):\n",
        "    return [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
        "\n",
        "def build_bigram_index(corpus):\n",
        "    bigram_index = defaultdict(set)\n",
        "    for doc_id, content in corpus.items():\n",
        "        tokens = tokenize_and_normalize(content)\n",
        "        bigrams = generate_bigrams(tokens)\n",
        "        for bigram in bigrams:\n",
        "            bigram_index[bigram].add(doc_id)\n",
        "    return {bigram: list(doc_ids) for bigram, doc_ids in bigram_index.items()}\n",
        "\n",
        "\n",
        "corpus = load_documents(file_paths)\n",
        "bigram_index = build_bigram_index(corpus)\n",
        "\n",
        "query = \"Harry Potter Magic\"\n",
        "query_tokens = tokenize_and_normalize(query)\n",
        "query_bigrams = generate_bigrams(query_tokens)\n",
        "related_docs = set()\n",
        "\n",
        "for bigram in query_bigrams:\n",
        "    if bigram in bigram_index:\n",
        "        related_docs.update(bigram_index[bigram])\n",
        "\n",
        "print(f\"Documents related to '{query}': {related_docs}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWC5c7XuabqJ",
        "outputId": "e124cdf8-9d10-4a16-9ec6-b38b68336dbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Suggested Terms for 'Magic': []\n"
          ]
        }
      ],
      "source": [
        "# Function to load documents from JSON files\n",
        "def load_documents(file_paths):\n",
        "    documents = {}\n",
        "    for path in file_paths:\n",
        "        with open(path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            for doc_id, content in data.items():\n",
        "                documents[doc_id] = ' '.join(content)  # Join list elements into a single string\n",
        "    return documents\n",
        "\n",
        "# Function to tokenize and normalize text\n",
        "def tokenize_and_normalize(text):\n",
        "    # Assuming word_tokenize and stop_words are already defined\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    filtered_tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Function to generate bigrams from tokens\n",
        "def generate_bigrams(tokens):\n",
        "    return [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
        "\n",
        "# Build a Bigram Index\n",
        "def build_bigram_index(corpus):\n",
        "    bigram_index = defaultdict(lambda: defaultdict(int))\n",
        "    for doc_id, content in corpus.items():\n",
        "        tokens = tokenize_and_normalize(content)\n",
        "        bigrams = generate_bigrams(tokens)\n",
        "        for bigram in bigrams:\n",
        "            bigram_index[bigram][doc_id] += 1\n",
        "    return bigram_index\n",
        "\n",
        "# Suggest Related Terms Based on Bigrams\n",
        "def suggest_related_terms(query, bigram_index, max_suggestions=5):\n",
        "    query_tokens = tokenize_and_normalize(query)\n",
        "    query_bigrams = generate_bigrams(query_tokens)\n",
        "    term_counts = defaultdict(int)\n",
        "\n",
        "    for bigram in query_bigrams:\n",
        "        if bigram in bigram_index:\n",
        "            for doc_id, count in bigram_index[bigram].items():\n",
        "                term_counts[bigram[1]] += count\n",
        "\n",
        "    # Sort terms by frequency and limit the number of suggestions\n",
        "    sorted_terms = sorted(term_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_terms[:max_suggestions]\n",
        "\n",
        "\n",
        "corpus = load_documents(file_paths)\n",
        "bigram_index = build_bigram_index(corpus)\n",
        "\n",
        "# Example query processing\n",
        "query = \"Magic\"\n",
        "suggestions = suggest_related_terms(query, bigram_index)\n",
        "print(f\"Suggested Terms for '{query}': {suggestions}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgEgeKnYb-Ts",
        "outputId": "27268c4d-e8ff-4e62-bb1e-51c60270b08f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Suggested Terms for 'Magic': [('harry', 8), ('mr', 3), ('come', 2), ('must', 1), ('leave', 1)]\n"
          ]
        }
      ],
      "source": [
        "def suggest_related_terms(query, bigram_index, max_suggestions=5):\n",
        "    query_tokens = tokenize_and_normalize(query)\n",
        "    query_bigrams = generate_bigrams(query_tokens)\n",
        "    term_counts = defaultdict(int)\n",
        "\n",
        "    # Look for direct bigram matches first\n",
        "    for bigram in query_bigrams:\n",
        "        if bigram in bigram_index:\n",
        "            for doc_id, count in bigram_index[bigram].items():\n",
        "                term_counts[bigram[1]] += count\n",
        "\n",
        "    # If no direct bigrams, look for any bigram containing the query terms\n",
        "    if not term_counts:\n",
        "        for token in query_tokens:\n",
        "            for bigram, docs in bigram_index.items():\n",
        "                if token in bigram:\n",
        "                    for doc_id, count in docs.items():\n",
        "                        term_counts[bigram[bigram.index(token) - 1]] += count\n",
        "\n",
        "    # Sort terms by frequency and limit the number of suggestions\n",
        "    sorted_terms = sorted(term_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_terms[:max_suggestions]\n",
        "\n",
        "# Re-run the example query processing\n",
        "suggestions = suggest_related_terms(\"Potter\", bigram_index)\n",
        "print(f\"Suggested Terms for 'Magic': {suggestions}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8e8pOLOdAt7"
      },
      "source": [
        "# Step 7: System Evaluation\n",
        "\n",
        "This step evaluates the search system using two key metrics:\n",
        "\n",
        "1. Precision@K: Measures the proportion of relevant results in the top K retrieved documents.\n",
        "2. Mean Reciprocal Rank (MRR): Evaluates the rank position of the first relevant result for each query.\n",
        "\n",
        "Visualizations (bar charts) are used to summarize the performance of the system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gJ8AohzCcVRq",
        "outputId": "fd567a11-c112-4aa0-aeb7-fb049aba5f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "Precision@1: 0.2500\n",
            "Precision@3: 0.0833\n",
            "Precision@5: 0.0500\n",
            "Mean Reciprocal Rank (MRR): 0.2500\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Function to calculate Precision@K\n",
        "def calculate_precision_at_k(suggestions, relevant, k):\n",
        "    \"\"\"\n",
        "    Calculates Precision@K for a given set of suggestions against relevant terms.\n",
        "    - suggestions: List of tuples (term, frequency) as suggestions.\n",
        "    - relevant: List of relevant terms.\n",
        "    - k: Integer, number of top suggestions to consider for calculation.\n",
        "    \"\"\"\n",
        "    # Extract just the terms from the top k suggestions for precision calculation\n",
        "    suggestion_terms = [term for term, _ in suggestions[:k]]\n",
        "    # Calculate how many of the top-k suggestions are relevant\n",
        "    relevant_count = sum(1 for term in suggestion_terms if term in relevant)\n",
        "    # Return the proportion of relevant suggestions\n",
        "    return relevant_count / k\n",
        "\n",
        "# Function to calculate Mean Reciprocal Rank (MRR)\n",
        "def calculate_mrr(suggestions, relevant):\n",
        "    \"\"\"\n",
        "    Calculates the Mean Reciprocal Rank (MRR) based on the suggestions and relevant terms.\n",
        "    - suggestions: List of tuples (term, frequency) as suggestions.\n",
        "    - relevant: List of relevant terms.\n",
        "    \"\"\"\n",
        "    # Extract just the terms from suggestions for MRR calculation\n",
        "    suggestion_terms = [term for term, _ in suggestions]\n",
        "    # Loop through suggestions to find the first relevant term\n",
        "    for index, term in enumerate(suggestion_terms, start=1):\n",
        "        if term in relevant:\n",
        "            # Return the reciprocal of the rank of the first relevant suggestion\n",
        "            return 1 / index\n",
        "    # If no relevant term is found, return 0\n",
        "    return 0\n",
        "\n",
        "# Example setup for evaluating the effectiveness of suggestions\n",
        "evaluation_data = [\n",
        "    {\"query\": \"harry\", \"relevant\": [\"potter\", \"ron\", \"hogwarts\"]},\n",
        "    {\"query\": \"dumbledore\", \"relevant\": [\"headmaster\", \"phoenix\", \"hogwarts\"]},\n",
        "    {\"query\": \"voldemort\", \"relevant\": [\"dark\", \"lord\", \"horcrux\"]},\n",
        "    {\"query\": \"magic\", \"relevant\": [\"wand\", \"spells\", \"power\"]},\n",
        "]\n",
        "\n",
        "# Prepare dictionary to hold precision and MRR for different k values\n",
        "k_values = [1, 3, 5]\n",
        "results = {\"precision_at_k\": {k: [] for k in k_values}, \"mrr\": []}\n",
        "\n",
        "# Process each evaluation case\n",
        "for data in evaluation_data:\n",
        "    query = data[\"query\"]  # Extract the query from the data\n",
        "    relevant = data[\"relevant\"]  # Extract the list of relevant terms\n",
        "    # Fetch suggestions using a bigram index (assumed to be defined elsewhere)\n",
        "    suggestions = suggest_related_terms(query, bigram_index)\n",
        "\n",
        "    # Calculate Precision@K for different k values\n",
        "    for k in k_values:\n",
        "        precision = calculate_precision_at_k(suggestions, relevant, k)\n",
        "        results[\"precision_at_k\"][k].append(precision)  # Store the precision for each k\n",
        "\n",
        "    # Calculate Mean Reciprocal Rank\n",
        "    mrr = calculate_mrr(suggestions, relevant)\n",
        "    results[\"mrr\"].append(mrr)  # Store the MRR result\n",
        "\n",
        "# Compute average results for precision at each k and MRR\n",
        "avg_precision_at_k = {k: sum(results[\"precision_at_k\"][k]) / len(evaluation_data) for k in k_values}\n",
        "avg_mrr = sum(results[\"mrr\"]) / len(evaluation_data)\n",
        "\n",
        "# Output the calculated averages\n",
        "print(\"\\nEvaluation Results:\")\n",
        "for k in k_values:\n",
        "    print(f\"Precision@{k}: {avg_precision_at_k[k]:.4f}\")\n",
        "print(f\"Mean Reciprocal Rank (MRR): {avg_mrr:.4f}\")\n",
        "\n",
        "# Plotting the results for Precision@K\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar([f\"P@{k}\" for k in k_values], [avg_precision_at_k[k] for k in k_values], color=\"skyblue\")\n",
        "plt.title(\"Precision@K\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.show()\n",
        "\n",
        "# Plotting the results for Mean Reciprocal Rank (MRR)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar([\"MRR\"], [avg_mrr], color=\"lightcoral\")\n",
        "plt.title(\"Mean Reciprocal Rank (MRR)\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh6AarqHgbZd"
      },
      "source": [
        "# Precision@K Chart<br>\n",
        "\n",
        "What is Precision@K?<br>\n",
        "- Precision@K measures the proportion of relevant suggestions in the top K suggestions.<br>\n",
        "\n",
        "- Precision@K = Relevant Suggestions in Top K / 𝐾 <br>\n",
        "\n",
        " - For example, if 3 out of the top 5 suggestions are relevant, Precision@5 = 3/5=0.6 <br>\n",
        "\n",
        "Observations from the Chart:<br>\n",
        "- Precision@1 (P@1): Approximately 0.25 (25% of the top-1 suggestions are relevant).<br>\n",
        "- Precision@3 (P@3): Approximately 0.083 (8.33% of the top-3 suggestions are relevant).<br>\n",
        "- Precision@5 (P@5): Approximately 0.05 (5% of the top-5 suggestions are relevant).<br>\n",
        "\n",
        "Interpretation:<br>\n",
        "- The system performs better at Precision@1 (first suggestion is relevant more often) but struggles to maintain relevance as more suggestions are included in the top K."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8SeXKYDggHv"
      },
      "source": [
        "# Mean Reciprocal Rank (MRR) Chart <br>\n",
        "\n",
        "What is MRR?<br>\n",
        "- MRR measures how quickly the system ranks a relevant suggestion.<br>\n",
        "- MRR = 1/𝑁∑(summation from N to i=1) 1 /Rank of First Relevant Suggestion in Query i<br>\n",
        "\n",
        "- For example:<br>\n",
        " - If the first relevant suggestion is at rank 1 for one query, it contributes 1/1 = 1.0<br>\n",
        " - If it is at rank 3 for another query, it contributes 1/3 = 0.33<br>\n",
        "\n",
        "Observations from the Chart:<br>\n",
        "- MRR is approximately 0.25 (indicating that, on average, the first relevant suggestion appears at rank 4).<br>\n",
        "\n",
        "Interpretation:<br>\n",
        "- The system rarely places the first relevant suggestion at the top rank, leading to a lower MRR score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FC9cxC6gk6P"
      },
      "source": [
        "# Overall Analysis <br>\n",
        "**Strengths:**<br>\n",
        "- The system performs decently for P@1, meaning it often suggests a relevant term as the first suggestion.<br>\n",
        "\n",
        "**Weaknesses:**\n",
        "- The precision drops significantly for P@3 and P@5, indicating that many irrelevant suggestions are included in the top ranks.<br>\n",
        "- A low MRR suggests that relevant terms are often buried deeper in the suggestion list.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7NYAP_Ngq0o"
      },
      "source": [
        "# Recommendations for Improvement <br>\n",
        "\n",
        "Improve Bigram Model:<br>\n",
        "- Enhance the bigram model by incorporating more contextual relationships.<br>\n",
        "- Include synonyms or semantic similarity in suggestion ranking.<br>\n",
        "\n",
        "Use Contextual Features:<br>\n",
        "- Incorporate sentence context or additional features (e.g., part-of-speech tagging).<br>\n",
        "\n",
        "Filter Suggestions:<br>\n",
        "- Apply stricter thresholds to exclude low-frequency or noisy bigrams.<br>\n",
        "\n",
        "Expand Training Data:<br>\n",
        "- Add more diverse and relevant examples to improve the model's vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1vEa3cteOvF",
        "outputId": "c31b9017-b3bb-4f2c-bb39-96cd7b19327b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Precision@5: 0.05\n",
            "Average MRR: 0.25\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'suggest_related_terms' outputs [(term, frequency), ...]\n",
        "def evaluate_bigram_suggestions(evaluation_data, bigram_index):\n",
        "    results = {\"precision\": [], \"mrr\": []}\n",
        "    k = 5  # Example k value for Precision@K\n",
        "\n",
        "    for data in evaluation_data:\n",
        "        query = data[\"query\"]\n",
        "        relevant_terms = data[\"relevant\"]\n",
        "\n",
        "        # Get top k suggestions\n",
        "        suggestions = suggest_related_terms(query, bigram_index)[:k]\n",
        "\n",
        "        # Extract only the terms from suggestions for comparison\n",
        "        suggested_terms = [term for term, freq in suggestions]\n",
        "\n",
        "        # Calculate Precision@K\n",
        "        precision = sum(1 for term in suggested_terms if term in relevant_terms) / k\n",
        "        results[\"precision\"].append(precision)\n",
        "\n",
        "        # Calculate MRR\n",
        "        mrr = 0\n",
        "        for i, term in enumerate(suggested_terms, start=1):\n",
        "            if term in relevant_terms:\n",
        "                mrr = 1 / i\n",
        "                break\n",
        "        results[\"mrr\"].append(mrr)\n",
        "\n",
        "    # Average the results\n",
        "    avg_precision = sum(results[\"precision\"]) / len(results[\"precision\"])\n",
        "    avg_mrr = sum(results[\"mrr\"]) / len(results[\"mrr\"])\n",
        "\n",
        "    print(f\"Average Precision@{k}: {avg_precision:.2f}\")\n",
        "    print(f\"Average MRR: {avg_mrr:.2f}\")\n",
        "\n",
        "# Example usage\n",
        "evaluate_bigram_suggestions(evaluation_data, bigram_index)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
